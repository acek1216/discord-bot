# -*- coding: utf-8 -*-
"""
çµ±ä¸€AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
"""

import asyncio
import functools
import time
from typing import Dict, Callable, Any, Optional, List
from dataclasses import dataclass
from abc import ABC, abstractmethod

from utils import safe_log
from rate_limiter import get_rate_limiter, rate_limited_request
from ai_config_loader import get_ai_config_loader, AIModelConfig

# AIClientConfig ã¯ ai_config_loader.AIModelConfig ã«ç§»è¡Œ
# å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹
AIClientConfig = AIModelConfig

class AIClientError(Exception):
    """AIé–¢é€£ã‚¨ãƒ©ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self, ai_name: str, message: str, original_error: Exception = None):
        self.ai_name = ai_name
        self.original_error = original_error
        super().__init__(f"{ai_name}ã‚¨ãƒ©ãƒ¼: {message}")

def with_ai_error_handling(ai_name: str, max_retries: int = 2):
    """AIã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿"""
    def decorator(func: Callable):
        @functools.wraps(func)
        async def wrapper(*args, **kwargs):
            last_error = None

            for attempt in range(max_retries + 1):
                try:
                    start_time = time.time()
                    result = await func(*args, **kwargs)
                    end_time = time.time()

                    # çµæœæ¤œè¨¼
                    if not result or not str(result).strip():
                        raise AIClientError(ai_name, "å¿œç­”ãŒç©ºã§ã—ãŸ")

                    # æˆåŠŸãƒ­ã‚°
                    if attempt > 0:
                        safe_log(f"âœ… {ai_name}å¾©æ—§æˆåŠŸ: ", f"è©¦è¡Œ{attempt + 1}å›ç›®ã§æˆåŠŸ ({end_time - start_time:.2f}s)")

                    return result

                except Exception as e:
                    last_error = e
                    if attempt < max_retries:
                        wait_time = 2 ** attempt  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                        safe_log(f"âš ï¸ {ai_name}ã‚¨ãƒ©ãƒ¼ï¼ˆè©¦è¡Œ{attempt + 1}ï¼‰: ", f"{str(e)[:100]}... {wait_time}ç§’å¾Œã«å†è©¦è¡Œ")
                        await asyncio.sleep(wait_time)
                    else:
                        safe_log(f"ğŸš¨ {ai_name}æœ€çµ‚ã‚¨ãƒ©ãƒ¼: ", e)

            # å…¨è©¦è¡Œå¤±æ•—æ™‚
            error_msg = str(last_error)[:200] if last_error else "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼"
            return f"{ai_name}ã‚¨ãƒ©ãƒ¼: {error_msg}"

        return wrapper
    return decorator

class AIClient(ABC):
    """AI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æŠ½è±¡åŸºåº•ã‚¯ãƒ©ã‚¹"""

    def __init__(self, config: AIModelConfig):
        self.config = config
        self.call_count = 0
        self.error_count = 0
        self.total_response_time = 0.0

    @abstractmethod
    async def generate(self, prompt: str, **kwargs) -> str:
        """AIå¿œç­”ã‚’ç”Ÿæˆ"""
        pass

    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        avg_time = self.total_response_time / max(self.call_count, 1)
        error_rate = self.error_count / max(self.call_count, 1)

        return {
            "name": self.config.name,
            "calls": self.call_count,
            "errors": self.error_count,
            "error_rate": f"{error_rate:.1%}",
            "avg_response_time": f"{avg_time:.2f}s"
        }

class OpenAIClient(AIClient):
    """OpenAIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    def __init__(self, config: AIModelConfig, openai_client, model: str = None):
        super().__init__(config)
        self.openai_client = openai_client
        self.model = model or config.model

    @with_ai_error_handling("OpenAI")
    async def generate(self, prompt: str, system_prompt: str = None, **kwargs) -> str:
        self.call_count += 1
        start_time = time.time()

        try:
            messages = []
            if system_prompt or self.config.system_prompt:
                messages.append({"role": "system", "content": system_prompt or self.config.system_prompt})
            messages.append({"role": "user", "content": prompt})

            # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦max_tokensã‹max_completion_tokensã‹ã‚’åˆ¤æ–­
            completion_params = {
                "model": self.model,
                "messages": messages,
                "temperature": self.config.temperature
            }

            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒ¢ãƒ‡ãƒ«åã‚’ãƒ­ã‚°å‡ºåŠ›
            print(f"ğŸ” ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {self.model}")

            # max_tokensã¨temperatureã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
            try:
                completion_params["max_tokens"] = 2000
                print(f"ğŸ”„ max_tokensè©¦è¡Œ: {2000}")
                response = await self.openai_client.chat.completions.create(**completion_params)
            except Exception as e:
                error_str = str(e)
                if "max_tokens" in error_str:
                    # max_tokensãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—ã§è©¦è¡Œ
                    print(f"ğŸ”„ max_tokensãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—ã§è©¦è¡Œ")
                    completion_params.pop("max_tokens", None)
                    try:
                        response = await self.openai_client.chat.completions.create(**completion_params)
                    except Exception as e2:
                        if "temperature" in str(e2):
                            print(f"ğŸ”„ temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚‚ãªã—ã§è©¦è¡Œ")
                            completion_params.pop("temperature", None)
                            response = await self.openai_client.chat.completions.create(**completion_params)
                        else:
                            raise e2
                elif "temperature" in error_str:
                    # temperatureãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—ã§è©¦è¡Œ
                    print(f"ğŸ”„ temperatureãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã—ã§è©¦è¡Œ")
                    completion_params.pop("temperature", None)
                    response = await self.openai_client.chat.completions.create(**completion_params)
                else:
                    raise e

            result = response.choices[0].message.content
            self.total_response_time += time.time() - start_time
            return result

        except Exception as e:
            self.error_count += 1
            raise e

class GeminiClient(AIClient):
    """Geminiç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    def __init__(self, config: AIModelConfig, generate_func: Callable):
        super().__init__(config)
        self.generate_func = generate_func

    @with_ai_error_handling("Gemini")
    async def generate(self, prompt: str, system_prompt: str = None, **kwargs) -> str:
        self.call_count += 1
        start_time = time.time()

        try:
            # system_promptãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯çµ±åˆ
            if system_prompt or self.config.system_prompt:
                full_prompt = f"{system_prompt or self.config.system_prompt}\n\n{prompt}"
            else:
                full_prompt = prompt

            result = await self.generate_func(full_prompt)
            self.total_response_time += time.time() - start_time
            return result

        except Exception as e:
            self.error_count += 1
            raise e

class ExternalAPIClient(AIClient):
    """å¤–éƒ¨APIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆClaude, Grok, Perplexityç­‰ï¼‰"""
    def __init__(self, config: AIModelConfig, api_func: Callable, api_key: str):
        super().__init__(config)
        self.api_func = api_func
        self.api_key = api_key

    @with_ai_error_handling("ExternalAPI")
    async def generate(self, prompt: str, **kwargs) -> str:
        self.call_count += 1
        start_time = time.time()

        try:
            result = await self.api_func(self.api_key, "user", prompt)
            self.total_response_time += time.time() - start_time
            return result

        except Exception as e:
            self.error_count += 1
            raise e

class AIClientManager:
    """çµ±ä¸€AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.clients: Dict[str, AIClient] = {}
        self.initialized = False

    def initialize(self, bot) -> None:
        """Botã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆYAMLè¨­å®šä½¿ç”¨ï¼‰"""
        if self.initialized:
            return

        # å¤–éƒ¨è¨­å®šãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        config_loader = get_ai_config_loader()
        ai_configs = config_loader.get_all_ai_configs()

        if not ai_configs:
            safe_log("ğŸ˜¨ AIè¨­å®šãŒç©ºã§ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™", "")
            config_loader.reload_config()  # ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’é©ç”¨
            ai_configs = config_loader.get_all_ai_configs()

        from ai_clients import (
            ask_gpt5, ask_gpt4o, ask_gpt5_mini, ask_gemini_2_5_pro,
            ask_claude, ask_grok, ask_llama, ask_lalah, ask_rekus, ask_o1_pro
        )

        # YAMLè¨­å®šã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å‹•çš„ç”Ÿæˆ
        self._create_clients_from_config(bot, ai_configs, {
            'ask_gpt5': ask_gpt5,
            'ask_gpt4o': ask_gpt4o,
            'ask_gpt5_mini': ask_gpt5_mini,
            'ask_gemini_2_5_pro': ask_gemini_2_5_pro,
            'ask_claude': ask_claude,
            'ask_grok': ask_grok,
            'ask_llama': ask_llama,
            'ask_lalah': ask_lalah,
            'ask_rekus': ask_rekus,
            'ask_o1_pro': ask_o1_pro
        })

        self.initialized = True
        safe_log("âœ… AIClientManageråˆæœŸåŒ–å®Œäº†: ", f"{len(self.clients)}å€‹ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç™»éŒ²")

    def _create_clients_from_config(self, bot, ai_configs: Dict[str, AIModelConfig], api_functions: Dict[str, Callable]):
        """è¨­å®šã‹ã‚‰ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å‹•çš„ç”Ÿæˆ"""
        for ai_type, config in ai_configs.items():
            try:
                if config.client_type == "openai":
                    # OpenAIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                    self.clients[ai_type] = OpenAIClient(
                        config,
                        bot.openai_client
                    )

                elif config.client_type == "gemini":
                    # Geminiç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                    # ãƒ¢ãƒ‡ãƒ«åã«ã‚ˆã£ã¦APIé–¢æ•°ã‚’é¸æŠ
                    if "2.5" in config.model:
                        api_func = api_functions.get('ask_gemini_2_5_pro')
                    elif "1.5" in config.model:
                        api_func = api_functions.get('ask_gemini_2_5_pro')
                    else:
                        api_func = api_functions.get('ask_gemini_2_5_pro')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

                    if api_func:
                        self.clients[ai_type] = GeminiClient(config, api_func)

                elif config.client_type == "external_api":
                    # å¤–éƒ¨APIç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                    api_func = api_functions.get(config.api_function)
                    if api_func and config.api_function == "ask_claude":
                        self.clients[ai_type] = ExternalAPIClient(
                            config, api_func, bot.openrouter_api_key
                        )
                    elif api_func and config.api_function == "ask_grok":
                        self.clients[ai_type] = ExternalAPIClient(
                            config, api_func, bot.grok_api_key
                        )
                    elif api_func and config.api_function == "ask_rekus":
                        # Perplexityç”¨
                        self.clients[ai_type] = GeminiClient(
                            config, lambda prompt: api_func(bot.perplexity_api_key, prompt)
                        )
                    elif api_func and config.api_function == "ask_lalah":
                        # Mistralç”¨
                        self.clients[ai_type] = GeminiClient(
                            config, lambda prompt: api_func(bot.mistral_client, prompt)
                        )

                elif config.client_type == "vertex_ai":
                    # Vertex AI (Llama)ç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                    if hasattr(bot, 'llama_model') and bot.llama_model:
                        api_func = api_functions.get('ask_llama')
                        if api_func:
                            self.clients[ai_type] = GeminiClient(
                                config,
                                lambda prompt: api_func(bot.llama_model, "llama_user", prompt)
                            )

                elif config.client_type == "mistral":
                    # Mistralç³»ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
                    api_func = api_functions.get('ask_lalah')
                    if api_func:
                        self.clients[ai_type] = GeminiClient(
                            config, lambda prompt: api_func(bot.mistral_client, prompt)
                        )

                if ai_type in self.clients:
                    safe_log(f"âœ… AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”Ÿæˆ: ", f"{ai_type} -> {config.name}")
                else:
                    safe_log(f"âš ï¸ AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç”Ÿæˆå¤±æ•—: ", f"{ai_type} ({config.client_type})")

            except Exception as e:
                safe_log(f"ğŸ˜¨ AIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ ({ai_type}): ", e)

        # æ—§ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰è¨­å®šã‚’å‰Šé™¤ã—ã€å‹•çš„è¨­å®šã«ç½®ãæ›ãˆ

    async def ask_ai(self, ai_type: str, prompt: str, priority: float = 1.0, **kwargs) -> str:
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»˜ãçµ±ä¸€AIå‘¼ã³å‡ºã—ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
        if not self.initialized:
            raise RuntimeError("AIClientManagerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        if ai_type not in self.clients:
            available = ", ".join(self.clients.keys())
            raise ValueError(f"ä¸æ˜ãªAIã‚¿ã‚¤ãƒ—: {ai_type}. åˆ©ç”¨å¯èƒ½: {available}")

        client = self.clients[ai_type]

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»˜ãã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
        service_name = self._get_service_name(ai_type)
        return await rate_limited_request(
            service_name,
            client.generate,
            prompt,
            priority=priority,
            **kwargs
        )

    def _get_service_name(self, ai_type: str) -> str:
        """AIã‚¿ã‚¤ãƒ—ã‹ã‚‰ã‚µãƒ¼ãƒ“ã‚¹åã‚’å–å¾—"""
        service_mapping = {
            "gpt5": "openai",
            "gpt4o": "openai",
            "gpt5mini": "openai",
            "gemini": "gemini",
            "claude": "claude",
            "grok": "grok",
            "mistral": "mistral",
            "llama": "llama",
            "perplexity": "perplexity",
            "o3": "openai",
            "genius": "openai"
        }
        return service_mapping.get(ai_type, ai_type)

    def get_available_ais(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªAIãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return list(self.clients.keys())

    def get_ai_info(self, ai_type: str) -> Dict[str, Any]:
        """AIæƒ…å ±ã‚’å–å¾—"""
        if ai_type not in self.clients:
            return {"error": "AI not found"}

        client = self.clients[ai_type]
        return {
            "name": client.config.name,
            "description": client.config.description,
            "supports_memory": client.config.supports_memory,
            "supports_attachments": client.config.supports_attachments,
            "stats": client.get_stats()
        }

    def get_all_stats(self) -> Dict[str, Dict[str, Any]]:
        """å…¨AIçµ±è¨ˆã‚’å–å¾—"""
        ai_stats = {ai_type: client.get_stats() for ai_type, client in self.clients.items()}

        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™çµ±è¨ˆã‚‚è¿½åŠ 
        rate_limiter = get_rate_limiter()
        rate_limit_stats = rate_limiter.get_all_stats()
        service_health = rate_limiter.get_service_health()

        return {
            "ai_performance": ai_stats,
            "rate_limits": rate_limit_stats,
            "service_health": service_health
        }

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
ai_manager = AIClientManager()

def get_ai_manager() -> AIClientManager:
    """AIãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    return ai_manager