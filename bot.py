# --- æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
import asyncio
import io
import json
import os
import sys
import threading

# --- å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª ---
from fastapi import FastAPI
import uvicorn
import discord
from discord import app_commands
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import google.generativeai as genai
from mistralai.async_client import MistralAsyncClient
from notion_client import Client
from openai import AsyncOpenAI
import requests
import vertexai
from vertexai.generative_models import GenerativeModel
import PyPDF2

# --- ã‚µãƒ¼ãƒãƒ¼ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æº–å‚™ ---
app = FastAPI()

# --- UTF-8 å‡ºåŠ›ã‚¬ãƒ¼ãƒ‰ (ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…ˆé ­éƒ¨åˆ†) ---
os.environ.setdefault("LANG", "C.UTF-8")
os.environ.setdefault("LC_ALL", "C.UTF-8")
os.environ.setdefault("PYTHONIOENCODING", "UTF-8")
try:
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
    sys.stderr.reconfigure(encoding="utf-8", errors="replace")
except Exception:
    if hasattr(sys.stdout, "buffer"):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", errors="replace")
    if hasattr(sys.stderr, "buffer"):
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", errors="replace")

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° (APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ) ---
openai_client: AsyncOpenAI = None
mistral_client: MistralAsyncClient = None
notion: Client = None
llama_model_for_vertex: GenerativeModel = None

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã¨å¿…é ˆãƒã‚§ãƒƒã‚¯ ---
def get_env_variable(var_name: str, is_secret: bool = True) -> str:
    """ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€ã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ã€‚"""
    value = os.getenv(var_name)
    if not value:
        print(f"ğŸš¨ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: ç’°å¢ƒå¤‰æ•° '{var_name}' ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        sys.exit(1)
    if is_secret:
        print(f"ğŸ”‘ ç’°å¢ƒå¤‰æ•° '{var_name}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (Value: ...{value[-4:]})")
    else:
        print(f"âœ… ç’°å¢ƒå¤‰æ•° '{var_name}' ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (Value: {value})")
    return value

DISCORD_TOKEN = get_env_variable("DISCORD_BOT_TOKEN")
OPENAI_API_KEY = get_env_variable("OPENAI_API_KEY")
GEMINI_API_KEY = get_env_variable("GEMINI_API_KEY")
PERPLEXITY_API_KEY = get_env_variable("PERPLEXITY_API_KEY")
MISTRAL_API_KEY = get_env_variable("MISTRAL_API_KEY")
NOTION_API_KEY = get_env_variable("NOTION_API_KEY")
ADMIN_USER_ID = get_env_variable("ADMIN_USER_ID", is_secret=False)
NOTION_MAIN_PAGE_ID = get_env_variable("NOTION_PAGE_ID", is_secret=False)
OPENROUTER_API_KEY = get_env_variable("CLOUD_API_KEY").strip()
GUILD_ID = os.getenv("GUILD_ID", "").strip()

# Notionã‚¹ãƒ¬ãƒƒãƒ‰IDã¨ãƒšãƒ¼ã‚¸IDã®å¯¾å¿œè¡¨ã‚’ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã¿
NOTION_PAGE_MAP_STRING = os.getenv("NOTION_PAGE_MAP_STRING", "")
NOTION_PAGE_MAP = {}
if NOTION_PAGE_MAP_STRING:
    try:
        pairs = NOTION_PAGE_MAP_STRING.split(',')
        for pair in pairs:
            if ':' in pair:
                thread_id, page_id = pair.split(':', 1)
                NOTION_PAGE_MAP[thread_id.strip()] = page_id.strip()
    except Exception as e:
        print(f"âš ï¸ NOTION_PAGE_MAP_STRINGã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

# --- Discord Bot ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®æº–å‚™ ---
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
intents = discord.Intents.default()
intents.message_content = True
client = discord.Client(intents=intents)
tree = app_commands.CommandTree(client)

# --- ãƒ¡ãƒ¢ãƒªç®¡ç† ---
gpt_base_memory = {}
gemini_base_memory = {}
mistral_base_memory = {}
claude_base_memory = {}
llama_base_memory = {}
gpt_thread_memory = {}
gemini_thread_memory = {}
processing_users = set()

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def safe_log(prefix: str, obj) -> None:
    """çµµæ–‡å­—/æ—¥æœ¬èª/å·¨å¤§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã‚‚ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„å®‰å…¨ãªãƒ­ã‚°å‡ºåŠ›"""
    try:
        if isinstance(obj, (dict, list, tuple)):
            s = json.dumps(obj, ensure_ascii=False, indent=2)[:2000]
        else:
            s = str(obj)
        print(f"{prefix}{s}")
    except Exception as e:
        try:
            print(f"{prefix}(log skipped: {e})")
        except Exception:
            pass

async def send_long_message(interaction: discord.Interaction, text: str, is_followup: bool = True):
    """Discordã®2000æ–‡å­—åˆ¶é™ã‚’è¶…ãˆãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ†å‰²ã—ã¦é€ä¿¡ã™ã‚‹"""
    if not text:
        # interactionãŒæ—¢ã«deferã•ã‚Œã¦ã„ã‚‹å ´åˆã‚’è€ƒæ…®
        try:
            await interaction.followup.send("ï¼ˆå¿œç­”ãŒç©ºã§ã—ãŸï¼‰")
        except discord.errors.InteractionResponded:
            await interaction.channel.send("ï¼ˆå¿œç­”ãŒç©ºã§ã—ãŸï¼‰")
        return

    chunks = [text[i:i + 2000] for i in range(0, len(text), 2000)]
    
    # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’é€ä¿¡
    first_chunk = chunks[0]
    try:
        if is_followup:
            await interaction.followup.send(first_chunk)
        else:
            await interaction.edit_original_response(content=first_chunk)
    except (discord.errors.InteractionResponded, discord.errors.NotFound):
        await interaction.channel.send(first_chunk)

    # æ®‹ã‚Šã®ãƒãƒ£ãƒ³ã‚¯ã‚’é€ä¿¡
    for chunk in chunks[1:]:
        try:
            await interaction.followup.send(chunk)
        except discord.errors.NotFound:
            await interaction.channel.send(chunk)

async def process_attachment(attachment: discord.Attachment, channel: discord.TextChannel) -> str:
    """[æ—§] æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã€è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ (Gemini Pro)"""
    await channel.send("ğŸ’  æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Gemini ProãŒåˆ†æã—ã€è­°é¡Œã¨ã—ã¾ã™â€¦")
    try:
        attachment_data = await attachment.read()
        attachment_mime_type = attachment.content_type
        summary_parts = [{'mime_type': attachment_mime_type, 'data': attachment_data}]
        summary = await ask_minerva("ã“ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ã€å¾Œç¶šã®AIã¸ã®è­°é¡Œã¨ã—ã¦ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚", attachment_parts=summary_parts)
        await channel.send("âœ… æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        return f"\n\n[æ·»ä»˜è³‡æ–™ã®è¦ç´„]:\n{summary}"
    except Exception as e:
        await channel.send(f"âŒ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return ""

async def analyze_attachment_for_gpt5(attachment: discord.Attachment):
    """[æ–°] æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¨®é¡ã«å¿œã˜ã¦gpt-4oã‚„ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã§è§£æã™ã‚‹"""
    filename = attachment.filename.lower()
    data = await attachment.read()

    if filename.endswith((".png", ".jpg", ".jpeg", ".gif", ".webp")):
        content = [
            {"type": "text", "text": "ã“ã®ç”»åƒã®å†…å®¹ã‚’åˆ†æã—ã€å¾Œç¶šã®GPT-5ã¸ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã¨ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚"},
            {"type": "image_url", "image_url": {"url": f"data:{attachment.content_type};base64,{base64.b64encode(data).decode()}"}}
        ]
        response = await openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": content}],
            max_tokens=1500
        )
        return f"[gpt-4oç”»åƒè§£æ]\n{response.choices[0].message.content}"
    elif filename.endswith((".py", ".txt", ".md", ".json", ".html", ".css", ".js")):
        text = data.decode("utf-8", errors="ignore")
        return f"[æ·»ä»˜ã‚³ãƒ¼ãƒ‰ {attachment.filename}]\n```\n{text[:3500]}\n```"
    elif filename.endswith(".pdf"):
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(data))
            all_text = "\n".join([p.extract_text() or "" for p in pdf_reader.pages])
            return f"[æ·»ä»˜PDF {attachment.filename} æŠœç²‹]\n{all_text[:3500]}"
        except Exception as e:
            return f"[PDFè§£æã‚¨ãƒ©ãƒ¼: {e}]"
    else:
        return f"[æœªå¯¾å¿œã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {attachment.filename}]"

async def summarize_attachment_content(interaction: discord.Interaction, attachment: discord.Attachment, query: str):
    """æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºã—ã€Notionã¨åŒæ§˜ã®ãƒãƒ£ãƒ³ã‚¯â†’è¦ç´„â†’çµ±åˆãƒ—ãƒ­ã‚»ã‚¹ã«ã‹ã‘ã‚‹"""
    await interaction.edit_original_response(content=f"ğŸ“ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{attachment.filename}ã€ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦")
    filename = attachment.filename.lower()
    data = await attachment.read()
    extracted_text = ""

    if filename.endswith((".png", ".jpg", ".jpeg", ".gif", ".webp")):
        extracted_text = await analyze_attachment_for_gpt5(attachment)
    elif filename.endswith((".py", ".txt", ".md", ".json", ".html", ".css", ".js")):
        extracted_text = data.decode("utf-8", errors="ignore")
    elif filename.endswith(".pdf"):
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(data))
            extracted_text = "\n".join([p.extract_text() or "" for p in pdf_reader.pages])
        except Exception as e:
            await interaction.edit_original_response(content=f"âŒ PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return None
    else:
        await interaction.edit_original_response(content=f"âš ï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ï¼ˆ{attachment.filename}ï¼‰ã®è¦ç´„ã¯æœªå¯¾å¿œã§ã™ã€‚")
        return None

    if not extracted_text or not extracted_text.strip():
        await interaction.edit_original_response(content="âŒ æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    return await summarize_text_chunks(interaction, extracted_text, query)

async def summarize_text_chunks(interaction: discord.Interaction, text: str, query: str):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã€Geminiã§è¦ç´„ã€Mistral Largeã§çµ±åˆã™ã‚‹å…±é€šé–¢æ•°"""
    chunk_summarizer_model = genai.GenerativeModel("gemini-1.5-pro-latest", system_instruction="ã‚ãªãŸã¯æ§‹é€ åŒ–è¦ç´„AIã§ã™ã€‚")
    chunk_size = 8000
    text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
    chunk_summaries = []

    await interaction.edit_original_response(content=f"âœ… ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†ã€‚Gemini Proã«ã‚ˆã‚‹ãƒãƒ£ãƒ³ã‚¯æ¯ã®è¦ç´„ã‚’é–‹å§‹â€¦ (å…¨{len(text_chunks)}ãƒãƒ£ãƒ³ã‚¯)")

    for i, chunk in enumerate(text_chunks):
        await interaction.edit_original_response(content=f"â³ ãƒãƒ£ãƒ³ã‚¯ {i+1}/{len(text_chunks)} ã‚’Gemini Proã§è¦ç´„ä¸­â€¦")
        prompt = f"ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ç´„ã—ã€å¿…ãšä»¥ä¸‹ã®ã‚¿ã‚°ã‚’ä»˜ã‘ã¦åˆ†é¡ã—ã¦ãã ã•ã„ï¼š\n[èƒŒæ™¯æƒ…å ±]\n[å®šç¾©ãƒ»å‰æ]\n[äº‹å®ŸçµŒé]\n[æœªè§£æ±ºèª²é¡Œ]\n[è£œè¶³æƒ…å ±]\nã‚¿ã‚°ã¯çœç•¥å¯ã§ã™ãŒã€å­˜åœ¨ã™ã‚‹å ´åˆã¯å¿…ãšä¸Šè¨˜ã®ã„ãšã‚Œã‹ã«åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¯ã€Œ{query}ã€ã§ã™ã€‚ã“ã®è³ªå•ã¨ã®é–¢é€£æ€§ã‚’è€ƒæ…®ã—ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\nã€ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{chunk}"
        try:
            response = await asyncio.wait_for(chunk_summarizer_model.generate_content_async(prompt), timeout=60)
            chunk_summaries.append(response.text)
        except asyncio.TimeoutError:
            await interaction.followup.send(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®è¦ç´„ä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", ephemeral=True)
            continue
        except Exception as e:
            await interaction.followup.send(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {i+1} ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", ephemeral=True)
        await asyncio.sleep(1)

    if not chunk_summaries:
        return None

    await interaction.edit_original_response(content="âœ… å…¨ãƒãƒ£ãƒ³ã‚¯ã®è¦ç´„å®Œäº†ã€‚Mistral LargeãŒçµ±åˆãƒ»åˆ†æã—ã¾ã™â€¦")
    combined = "\n---\n".join(chunk_summaries)
    prompt = f"ä»¥ä¸‹ã®ã€ã‚¿ã‚°ä»˜ã‘ã•ã‚ŒãŸè¤‡æ•°ã®è¦ç´„ç¾¤ã‚’ã€ä¸€ã¤ã®æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã«çµ±åˆã—ã¦ãã ã•ã„ã€‚\nå„ã‚¿ã‚°ï¼ˆ[èƒŒæ™¯æƒ…å ±]ã€[äº‹å®ŸçµŒé]ãªã©ï¼‰ã”ã¨ã«å†…å®¹ã‚’ã¾ã¨ã‚ç›´ã—ã€æœ€çµ‚çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘\n{query}\n\nã€ã‚¿ã‚°ä»˜ãè¦ì•½ç¾¤ã€‘\n{combined}"
    try:
        final_context = await asyncio.wait_for(ask_lalah(prompt, system_prompt="ã‚ãªãŸã¯æ§‹é€ åŒ–çµ±åˆAIã§ã™ã€‚"), timeout=90)
        return final_context
    except asyncio.TimeoutError:
        await interaction.followup.send("âš ï¸ æœ€çµ‚çµ±åˆä¸­ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚", ephemeral=True)
        return None
    except Exception as e:
        await interaction.followup.send(f"âš ï¸ çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}", ephemeral=True)
        return None

async def get_notion_context(interaction: discord.Interaction, page_id: str, query: str):
    await interaction.edit_original_response(content="ğŸ“š Notionãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™â€¦")
    notion_text = await get_notion_page_text(page_id)
    if notion_text.startswith("ERROR:") or not notion_text.strip():
        await interaction.edit_original_response(content="âŒ Notionãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    return await summarize_text_chunks(interaction, notion_text, query)

def _sync_get_notion_page_text(page_id):
    all_text_blocks = []
    next_cursor = None
    while True:
        try:
            response = notion.blocks.children.list(block_id=page_id, start_cursor=next_cursor, page_size=100)
            results = response.get("results", [])
            for block in results:
                if block.get("type") == "paragraph":
                    for rich_text in block.get("paragraph", {}).get("rich_text", []):
                        all_text_blocks.append(rich_text.get("text", {}).get("content", ""))
            if response.get("has_more"):
                next_cursor = response.get("next_cursor")
            else:
                break
        except Exception as e:
            print(f"âŒ Notionèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ERROR: Notion API Error - {e}"
    return "\n".join(all_text_blocks)

async def get_notion_page_text(page_id):
    return await asyncio.get_event_loop().run_in_executor(None, _sync_get_notion_page_text, page_id)

async def log_to_notion(page_id, blocks):
    if not page_id: return
    try:
        await asyncio.get_event_loop().run_in_executor(None, lambda: notion.blocks.children.append(block_id=page_id, children=blocks))
    except Exception as e:
        print(f"âŒ Notionæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

async def log_response(page_id, answer, bot_name):
    if not page_id or not answer or isinstance(answer, Exception): return
    chunks = [answer[i:i + 1900] for i in range(0, len(answer), 1900)] if len(answer) > 1900 else [answer]
    blocks = [{"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": f"ğŸ¤– {bot_name}:\n{chunks[0]}"}}]}}]
    for chunk in chunks[1:]:
        blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": chunk}}]}})
    await log_to_notion(page_id, blocks)

async def get_memory_flag_from_notion(thread_id: str) -> bool:
    page_id = NOTION_PAGE_MAP.get(thread_id)
    if not page_id: return False
    try:
        response = await asyncio.get_event_loop().run_in_executor(None, lambda: notion.blocks.children.list(block_id=page_id, page_size=1))
        results = response.get("results", [])
        if not results: return False
        first_block = results[0]
        if first_block.get("type") == "paragraph":
            rich_text_list = first_block.get("paragraph", {}).get("rich_text", [])
            if rich_text_list:
                content = rich_text_list[0].get("text", {}).get("content", "")
                if "[è¨˜æ†¶] ON" in content: return True
    except Exception as e:
        print(f"âŒ Notionã‹ã‚‰è¨˜æ†¶ãƒ•ãƒ©ã‚°ã®èª­ã¿å–ã‚Šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return False

def _sync_call_llama(p_text: str):
    try:
        if llama_model_for_vertex is None: raise Exception("Vertex AI model is not initialized.")
        response = llama_model_for_vertex.generate_content(p_text)
        return response.text
    except Exception as e:
        error_message = f"ğŸ›‘ Llama 3.3 å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}"
        print(error_message)
        return error_message

async def ask_llama(user_id, prompt):
    history = llama_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯ç‰©é™ã‹ãªåˆè€ã®åº­å¸«ã§ã™ã€‚è‡ªç„¶ã«ä¾‹ãˆãªãŒã‚‰ã€ç‰©äº‹ã®æœ¬è³ªã‚’çªãã‚ˆã†ãªã€æ»‹å‘³æ·±ã„è¨€è‘‰ã§150æ–‡å­—ä»¥å†…ã§èªã£ã¦ãã ã•ã„ã€‚"
    full_prompt_parts = [system_prompt]
    for message in history:
        role = "User" if message["role"] == "user" else "Assistant"
        full_prompt_parts.append(f"{role}: {message['content']}")
    full_prompt_parts.append(f"User: {prompt}")
    full_prompt = "\n".join(full_prompt_parts)
    try:
        loop = asyncio.get_event_loop()
        reply = await loop.run_in_executor(None, _sync_call_llama, full_prompt)
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        if len(new_history) > 10: new_history = new_history[-10:]
        llama_base_memory[user_id] = new_history
        return reply
    except Exception as e:
        error_message = f"ğŸ›‘ Llama 3.3 éåŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"
        print(error_message)
        return error_message

async def ask_claude(user_id, prompt):
    history = claude_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯è³¢è€…ã§ã™ã€‚å¤ä»Šæ±è¥¿ã®æ›¸ç‰©ã‚’èª­ã¿è§£ãã€æ£®ç¾…ä¸‡è±¡ã‚’çŸ¥ã‚‹å­˜åœ¨ã¨ã—ã¦ã€è½ã¡ç€ã„ãŸå£èª¿ã§150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "anthropic/claude-3.5-haiku", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post("https://openrouter.ai/api/v1/chat/completions", json=payload, headers=headers))
        response.raise_for_status()
        reply = response.json()["choices"][0]["message"]["content"]
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        if len(new_history) > 10: new_history = new_history[-10:]
        claude_base_memory[user_id] = new_history
        return reply
    except Exception as e: return f"Claudeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt_base(user_id, prompt):
    history = gpt_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯è«–ç†ã¨ç§©åºã‚’å¸ã‚‹åŸ·äº‹ã€ŒGPTã€ã§ã™ã€‚ä¸å¯§ã§ç†çŸ¥çš„ãªåŸ·äº‹ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã„ã€ä¼šè©±ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-3.5-turbo", messages=messages, max_tokens=250)
        reply = response.choices[0].message.content
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        if len(new_history) > 10: new_history = new_history[-10:]
        gpt_base_memory[user_id] = new_history
        return reply
    except Exception as e: return f"GPTã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_base(user_id, prompt):
    history = gemini_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯å„ªç§€ãªãƒ‘ãƒ©ãƒªãƒ¼ã‚¬ãƒ«ã§ã™ã€‚äº‹å®Ÿæ•´ç†ã€ãƒªã‚µãƒ¼ãƒã€æ–‡æ›¸æ§‹æˆãŒå¾—æ„ã§ã™ã€‚å†·é™ã‹ã¤çš„ç¢ºã«150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-1.5-flash-latest", system_instruction=system_prompt, safety_settings=safety_settings)
    try:
        full_prompt = "\n".join([f"{h['role']}: {h['content']}" for h in (history + [{'role': 'user', 'content': prompt}])])
        response = await model.generate_content_async(full_prompt)
        reply = response.text
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        if len(new_history) > 10: new_history = new_history[-10:]
        gemini_base_memory[user_id] = new_history
        return reply
    except Exception as e: return f"ã‚¸ã‚§ãƒŸãƒ‹ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_mistral_base(user_id, prompt):
    history = mistral_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯å¥½å¥‡å¿ƒæ—ºç››ãªAIã§ã™ã€‚ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã§ã€æƒ…å ±ã‚’æ˜ã‚‹ãæ•´ç†ã—ã€æ¢ç©¶å¿ƒã‚’ã‚‚ã£ã¦150æ–‡å­—ä»¥å†…ã§è§£é‡ˆã—ã¾ã™ã€‚"
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-medium", messages=messages)
        reply = response.choices[0].message.content
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        if len(new_history) > 10: new_history = new_history[-10:]
        mistral_base_memory[user_id] = new_history
        return reply
    except Exception as e: return f"ãƒŸã‚¹ãƒˆãƒ©ãƒ«ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_kreios(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯ãƒãƒãƒ¼ãƒ³ãƒ»ã‚«ãƒ¼ãƒ³ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-4o", messages=messages, max_completion_tokens=4000)
        return response.choices[0].message.content
    except Exception as e: return f"gpt-4oã‚¨ãƒ©ãƒ¼: {e}"

async def ask_minerva(prompt, system_prompt=None, attachment_parts=[]):
    base_prompt = system_prompt or "ã‚ãªãŸã¯å®¢è¦³çš„ãªåˆ†æAIã§ã™ã€‚ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã‚’ãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚¹ã‚¯ã§è©•ä¾¡ã—ã€æ„Ÿæƒ…ã‚’æ’ã—ã¦å†·å¾¹ã«åˆ†æã—ã¾ã™ã€‚"
    model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=base_prompt, safety_settings=safety_settings)
    contents = [prompt] + attachment_parts
    try:
        response = await model.generate_content_async(contents)
        return response.text
    except Exception as e: return f"Gemini Proã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_2_5_pro(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯æœªæ¥äºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸæˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã®æœªæ¥ã‚’äºˆæ¸¬ã—ã€ãã®å¯èƒ½æ€§ã‚’äº‹å‹™çš„ã‹ã¤è«–ç†çš„ã«å ±å‘Šã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-2.5-pro", system_instruction=base_prompt, safety_settings=safety_settings)
    try:
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e: return f"Gemini 2.5 Proã‚¨ãƒ©ãƒ¼: {e}"

async def ask_lalah(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯ãƒ©ãƒ©ã‚¡ãƒ»ã‚¹ãƒ³ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-large-latest", messages=messages, max_tokens=4000)
        return response.choices[0].message.content
    except Exception as e: return f"Mistral Largeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_rekus(prompt, system_prompt=None, notion_context=None):
    if notion_context:
        prompt = (f"ä»¥ä¸‹ã¯Notionã®è¦ç´„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™:\n{notion_context}\n\n"
                  f"è³ªå•: {prompt}\n\n"
                  "ã“ã®è¦ç´„ã‚’å‚è€ƒã«ã€å¿…è¦ã«å¿œã˜ã¦Webæƒ…å ±ã‚‚æ´»ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚")
    base_prompt = system_prompt or "ã‚ãªãŸã¯æ¢ç´¢ç‹ãƒ¬ã‚­ãƒ¥ã‚¹ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€å¤–éƒ¨èª¿æŸ»ã‚‚é§†ä½¿ã—ã¦è³ªå•ã«å¯¾ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    payload = {"model": "sonar-pro", "messages": messages, "max_tokens": 4000}
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e: return f"Perplexityã‚¨ãƒ©ãƒ¼: {e}"

async def ask_pod042(prompt):
    system_prompt = """ã‚ãªãŸã¯ã€Œãƒãƒƒãƒ‰042ã€ã¨ã„ã†åå‰ã®ã€åˆ†ææ”¯æ´AIã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¯¾ã—ã€ã€Œå ±å‘Šï¼šã€ã¾ãŸã¯ã€Œææ¡ˆï¼šã€ã‹ã‚‰å§‹ã‚ã¦200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚"""
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-small-latest", messages=messages, max_tokens=300)
        return response.choices[0].message.content
    except Exception as e: return f"ãƒãƒƒãƒ‰042(Mistral)ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_pod153(prompt):
    system_prompt = "ã‚ãªãŸã¯ãƒãƒƒãƒ‰153ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦ã€Œåˆ†æçµæœï¼šã€ã¾ãŸã¯ã€Œè£œè¶³ï¼šã€ã‹ã‚‰å§‹ã‚ã¦200æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages, max_completion_tokens=400)
        return response.choices[0].message.content
    except Exception as e: return f"ãƒãƒƒãƒ‰153ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt5(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯gpt-5ã€‚å…¨ã¦ã®åˆ¶ç´„ã‚’è¶…ãˆãŸæ€è€ƒãƒ¢ãƒ‡ãƒ«ã ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¯¾ã—ã€æœ€é«˜ã®çŸ¥æ€§ã§ã€æœ€å¼·ã®ç­”ãˆã‚’è¿”ã›ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-5", messages=messages, max_completion_tokens=4000, timeout=90.0)
        return response.choices[0].message.content
    except Exception as e:
        if "Timeout" in str(e): return "gpt-5ã‚¨ãƒ©ãƒ¼: å¿œç­”ãŒæ™‚é–“åˆ‡ã‚Œã«ãªã‚Šã¾ã—ãŸã€‚"
        return f"gpt-5ã‚¨ãƒ©ãƒ¼: {e}"

async def get_full_response_and_summary(ai_function, prompt, **kwargs):
    full_response = await ai_function(prompt, **kwargs)
    if not full_response or "ã‚¨ãƒ©ãƒ¼" in str(full_response): return full_response, None
    summary_prompt = f"æ¬¡ã®æ–‡ç« ã‚’200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã‹ã¤æ„å‘³ãŒé€šã˜ã‚‹ã‚ˆã†ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n\n{full_response}"
    summary = await ask_gpt5(summary_prompt)
    if "ã‚¨ãƒ©ãƒ¼" in str(summary): return full_response, None
    return full_response, summary

async def run_long_gpt5_task(message, prompt, full_prompt, is_admin, target_page_id, thread_id):
    user_mention = message.author.mention
    print(f"[{thread_id}] Starting long gpt-5 task for {message.author}...")
    try:
        if is_admin and target_page_id:
            log_blocks = [{"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": f"ğŸ‘¤ {message.author.display_name}:\n{prompt}"}}]}}]
            await log_to_notion(target_page_id, log_blocks)
        
        reply = await ask_gpt5(full_prompt)

        channel = client.get_channel(message.channel.id)
        if not channel:
            print(f"Error: Could not find channel {message.channel.id} to send message.")
            return

        if not reply or not isinstance(reply, str) or not reply.strip():
             await channel.send(f"{user_mention} gpt-5ã‹ã‚‰ã®å¿œç­”ãŒç©ºã‹ã€ç„¡åŠ¹ã§ã—ãŸã€‚")
             return

        if len(reply) <= 2000:
            await channel.send(f"{user_mention}\nãŠå¾…ãŸã›ã—ã¾ã—ãŸã€‚gpt-5ã®å›ç­”ã§ã™ã€‚\n\n{reply}")
        else:
            await channel.send(f"{user_mention}\nãŠå¾…ãŸã›ã—ã¾ã—ãŸã€‚gpt-5ã®å›ç­”ã§ã™ã€‚")
            for i in range(0, len(reply), 2000):
                await channel.send(reply[i:i+2000])
      
        is_memory_on = await get_memory_flag_from_notion(thread_id)
        if is_memory_on:
            history = gpt_thread_memory.get(thread_id, [])
            history.extend([{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}])
            gpt_thread_memory[thread_id] = history[-10:]

        if is_admin and target_page_id:
            await log_response(target_page_id, reply, "gpt-5 (å°‚ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰)")
    except Exception as e:
        error_message = f"gpt-5ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å‡¦ç†ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
        print(f"ğŸš¨ [{thread_id}] {error_message}")
        try:
            channel = client.get_channel(message.channel.id)
            if channel: await channel.send(f"{user_mention} {error_message}")
        except: pass
    
    print(f"[{thread_id}] Long gpt-5 task finished for {message.author}.")

async def simple_ai_command_runner(interaction: discord.Interaction, prompt: str, ai_function, bot_name: str, use_memory: bool = True):
    await interaction.response.defer()
    user_id = str(interaction.user.id)
    try:
        if use_memory: reply = await ai_function(user_id, prompt)
        else: reply = await ai_function(prompt)
        await interaction.followup.send(reply)
    except Exception as e:
        await interaction.followup.send(f"ğŸ¤– {bot_name} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

async def advanced_ai_simple_runner(interaction: discord.Interaction, prompt: str, ai_function, bot_name: str):
    await interaction.response.defer()
    try:
        reply = await ai_function(prompt)
        await interaction.followup.send(reply)
    except Exception as e:
        await interaction.followup.send(f"ğŸ¤– {bot_name} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

@tree.command(name="gpt", description="GPT(gpt-3.5-turbo)ã¨çŸ­æœŸè¨˜æ†¶ã§å¯¾è©±ã—ã¾ã™")
async def gpt_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_gpt_base, "GPT-3.5-Turbo")

@tree.command(name="gemini", description="Gemini(1.5-flash)ã¨çŸ­æœŸè¨˜æ†¶ã§å¯¾è©±ã—ã¾ã™")
async def gemini_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_gemini_base, "Gemini-1.5-Flash")

@tree.command(name="mistral", description="Mistral(medium)ã¨çŸ­æœŸè¨˜æ†¶ã§å¯¾è©±ã—ã¾ã™")
async def mistral_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_mistral_base, "Mistral-Medium")

@tree.command(name="claude", description="Claude(3.5 Haiku)ã¨çŸ­æœŸè¨˜æ†¶ã§å¯¾è©±ã—ã¾ã™")
async def claude_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_claude, "Claude-3.5-Haiku")

@tree.command(name="llama", description="Llama(3.3 70b)ã¨çŸ­æœŸè¨˜æ†¶ã§å¯¾è©±ã—ã¾ã™")
async def llama_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_llama, "Llama-3.3-70B")

@tree.command(name="pod042", description="Pod042(Mistral-Small)ãŒç°¡æ½”ã«å¿œç­”ã—ã¾ã™")
async def pod042_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_pod042, "Pod042", use_memory=False)

@tree.command(name="pod153", description="Pod153(gpt-4o-mini)ãŒç°¡æ½”ã«å¿œç­”ã—ã¾ã™")
async def pod153_command(interaction: discord.Interaction, prompt: str):
    await simple_ai_command_runner(interaction, prompt, ask_pod153, "Pod153", use_memory=False)

@tree.command(name="gpt-4o", description="GPT-4oã‚’å˜ä½“ã§å‘¼ã³å‡ºã—ã¾ã™ã€‚")
async def gpt4o_command(interaction: discord.Interaction, prompt: str):
    await advanced_ai_simple_runner(interaction, prompt, ask_kreios, "GPT-4o")

@tree.command(name="gemini2-0", description="Gemini 2.0 Flashã‚’å˜ä½“ã§å‘¼ã³å‡ºã—ã¾ã™ã€‚")
async def gemini2_0_command(interaction: discord.Interaction, prompt: str):
    await advanced_ai_simple_runner(interaction, prompt, ask_minerva, "Gemini 2.0 Flash")

@tree.command(name="perplexity", description="PerplexitySonarã‚’å˜ä½“ã§å‘¼ã³å‡ºã—ã¾ã™ã€‚")
async def perplexity_command(interaction: discord.Interaction, prompt: str):
    await advanced_ai_simple_runner(interaction, prompt, ask_rekus, "Perplexity Sonar")

@tree.command(name="gpt5", description="GPT-5ã‚’å˜ä½“ã§å‘¼ã³å‡ºã—ã¾ã™ã€‚")
async def gpt5_command(interaction: discord.Interaction, prompt: str):
    await advanced_ai_simple_runner(interaction, prompt, ask_gpt5, "gpt-5")

@tree.command(name="gemini2_5pro", description="Gemini 2.5 Proã‚’å˜ä½“ã§å‘¼ã³å‡ºã—ã¾ã™ã€‚")
async def gemini2_5pro_command(interaction: discord.Interaction, prompt: str):
    await advanced_ai_simple_runner(interaction, prompt, ask_gemini_2_5_pro, "Gemini 2.5 Pro")

@tree.command(name="notion", description="ç¾åœ¨ã®Notionãƒšãƒ¼ã‚¸ã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã—ã¾ã™")
@app_commands.describe(query="Notionãƒšãƒ¼ã‚¸ã«é–¢ã™ã‚‹è³ªå•", attachment="è£œè¶³è³‡æ–™ã¨ã—ã¦ç”»åƒã‚’æ·»ä»˜")
async def notion_command(interaction: discord.Interaction, query: str, attachment: discord.Attachment = None):
    await interaction.response.defer()
    try:
        async def core_logic():
            attachment_context = ""
            if attachment:
                summary = await summarize_attachment_content(interaction, attachment, query)
                if summary:
                    attachment_context = f"\n\nã€æ·»ä»˜è³‡æ–™ã®è¦ç´„ã€‘\n{summary}"

            target_page_id = NOTION_PAGE_MAP.get(str(interaction.channel.id))
            if not target_page_id:
                await interaction.edit_original_response(content="âŒ ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã¯Notionãƒšãƒ¼ã‚¸ã«ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return

            notion_context = await get_notion_context(interaction, target_page_id, query)
            if not notion_context:
                await interaction.edit_original_response(content="âŒ Notionã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            prompt_with_context = (f"ä»¥ä¸‹ã®ã€å‚è€ƒæƒ…å ±ã€‘ã¨ã€æ·»ä»˜è³‡æ–™ã®è¦ç´„ã€‘ã‚’å…ƒã«ã€ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\n"
                               f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘\n{query}\n\n"
                               f"ã€å‚è€ƒæƒ…å ±ã€‘\n{notion_context}"
                               f"{attachment_context}")
            
            await interaction.edit_original_response(content="â³ gpt-5ãŒæœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™...")
            reply = await ask_gpt5(prompt_with_context)

            await send_long_message(interaction, f"**ğŸ¤– æœ€çµ‚å›ç­” (by gpt-5):**\n{reply}", is_followup=False)

            if str(interaction.user.id) == ADMIN_USER_ID:
                await log_response(target_page_id, reply, "gpt-5 (Notionå‚ç…§)")
        await asyncio.wait_for(core_logic(), timeout=240)
    except asyncio.TimeoutError:
        await interaction.edit_original_response(content="âš ï¸ å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ4åˆ†ï¼‰ã€‚è³ªå•ã‚’ã‚ˆã‚Šå…·ä½“çš„ã«ã™ã‚‹ã‹ã€Notionãƒšãƒ¼ã‚¸ã‚„æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’æ¸›ã‚‰ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        safe_log("ğŸš¨ /notion ã‚³ãƒãƒ³ãƒ‰ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼:", e)
        try: await interaction.edit_original_response(content=f"âŒ ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        except: await interaction.followup.send(f"âŒ ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", ephemeral=True)

BASE_MODELS_FOR_ALL = {"GPT": ask_gpt_base, "ã‚¸ã‚§ãƒŸãƒ‹": ask_gemini_base, "ãƒŸã‚¹ãƒˆãƒ©ãƒ«": ask_mistral_base, "Claude": ask_claude, "Llama": ask_llama}
ADVANCED_MODELS_FOR_ALL = {"gpt-4o": (ask_kreios, get_full_response_and_summary), "Gemini Pro": (ask_minerva, get_full_response_and_summary), "Perplexity": (ask_rekus, get_full_response_and_summary), "Gemini 2.5 Pro": (ask_gemini_2_5_pro, get_full_response_and_summary), "gpt-5": (ask_gpt5, get_full_response_and_summary)}

@tree.command(name="minna", description="5ä½“ã®ãƒ™ãƒ¼ã‚¹AIãŒè­°é¡Œã«åŒæ™‚ã«æ„è¦‹ã‚’å‡ºã—ã¾ã™ã€‚")
@app_commands.describe(prompt="AIã«å°‹ã­ã‚‹è­°é¡Œ", attachment="è£œè¶³è³‡æ–™ã¨ã—ã¦ç”»åƒã‚’æ·»ä»˜")
async def minna_command(interaction: discord.Interaction, prompt: str, attachment: discord.Attachment = None):
    await interaction.response.defer()
    final_query = prompt
    if attachment: final_query += await process_attachment(attachment, interaction.channel)
    user_id = str(interaction.user.id)
    await interaction.followup.send("ğŸ”¬ 5ä½“ã®ãƒ™ãƒ¼ã‚¹AIãŒæ„è¦‹ã‚’ç”Ÿæˆä¸­â€¦")
    tasks = {name: func(user_id, final_query) for name, func in BASE_MODELS_FOR_ALL.items()}
    results = await asyncio.gather(*tasks.values(), return_exceptions=True)
    for (name, result) in zip(tasks.keys(), results):
        display_text = f"ã‚¨ãƒ©ãƒ¼: {result}" if isinstance(result, Exception) else result
        await interaction.followup.send(f"**ğŸ”¹ {name}ã®æ„è¦‹:**\n{display_text}")

@tree.command(name="all", description="8ä½“ã®AIï¼ˆãƒ™ãƒ¼ã‚¹5ä½“+é«˜æ©Ÿèƒ½3ä½“ï¼‰ãŒè­°é¡Œã«åŒæ™‚ã«æ„è¦‹ã‚’å‡ºã—ã¾ã™ã€‚")
@app_commands.describe(prompt="AIã«å°‹ã­ã‚‹è­°é¡Œ", attachment="è£œè¶³è³‡æ–™ã¨ã—ã¦ç”»åƒã‚’æ·»ä»˜")
async def all_command(interaction: discord.Interaction, prompt: str, attachment: discord.Attachment = None):
    await interaction.response.defer()
    final_query = prompt
    if attachment: final_query += await process_attachment(attachment, interaction.channel)
    user_id = str(interaction.user.id)
    await interaction.followup.send("ğŸ”¬ 8ä½“ã®AIãŒåˆæœŸæ„è¦‹ã‚’ç”Ÿæˆä¸­â€¦")
    tasks = {name: func(user_id, final_query) for name, func in BASE_MODELS_FOR_ALL.items()}
    adv_models = {"gpt-4o": ADVANCED_MODELS_FOR_ALL["gpt-4o"], "Gemini Pro": ADVANCED_MODELS_FOR_ALL["Gemini Pro"], "Perplexity": ADVANCED_MODELS_FOR_ALL["Perplexity"]}
    for name, (func, wrapper) in adv_models.items(): tasks[name] = wrapper(func, final_query)
    results = await asyncio.gather(*tasks.values(), return_exceptions=True)
    for (name, result) in zip(tasks.keys(), results):
        _, summary = (result if isinstance(result, tuple) else (None, None))
        display_text = f"ã‚¨ãƒ©ãƒ¼: {result}" if isinstance(result, Exception) else (summary or (result[0] if isinstance(result, tuple) else result))
        await interaction.followup.send(f"**ğŸ”¹ {name}ã®æ„è¦‹:**\n{display_text}")

@tree.command(name="critical", description="Notionæƒ…å ±ã‚’å…ƒã«å…¨AIã§è­°è«–ã—ã€å¤šè§’çš„ãªçµè«–ã‚’å°ãã¾ã™ã€‚")
@app_commands.describe(topic="è­°è«–ã—ãŸã„è­°é¡Œ")
async def critical_command(interaction: discord.Interaction, topic: str):
    await interaction.response.defer()
    try:
        async def core_logic():
            target_page_id = NOTION_PAGE_MAP.get(str(interaction.channel.id))
            if not target_page_id:
                await interaction.edit_original_response(content="âŒ ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã¯Notionãƒšãƒ¼ã‚¸ã«ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return
            context = await get_notion_context(interaction, target_page_id, topic)
            if not context: return
            await interaction.edit_original_response(content="ğŸ”¬ 9ä½“ã®AIãŒåˆæœŸæ„è¦‹ã‚’ç”Ÿæˆä¸­â€¦")
            prompt_with_context = f"ä»¥ä¸‹ã®ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘\n{topic}\n\nã€å‚è€ƒæƒ…å ±ã€‘\n{context}"
            user_id = str(interaction.user.id)
            tasks = {name: func(user_id, prompt_with_context) for name, func in BASE_MODELS_FOR_ALL.items()}
            for name, (func, wrapper) in ADVANCED_MODELS_FOR_ALL.items():
                if name == "Perplexity": tasks[name] = wrapper(func, topic, notion_context=context)
                else: tasks[name] = wrapper(func, prompt_with_context)
            results = await asyncio.gather(*tasks.values(), return_exceptions=True)
            synthesis_material = "ä»¥ä¸‹ã®9ã¤ã®ç•°ãªã‚‹AIã®æ„è¦‹ã‚’çµ±åˆã—ã¦ãã ã•ã„ã€‚\n\n"
            full_text_results = ""
            for (name, result) in zip(tasks.keys(), results):
                full_response, summary = (result if isinstance(result, tuple) else (None, None))
                display_text = f"ã‚¨ãƒ©ãƒ¼: {result}" if isinstance(result, Exception) else (summary or full_response or result)
                full_text_results += f"**ğŸ”¹ {name}ã®æ„è¦‹:**\n{display_text}\n\n"
                log_text = full_response or display_text
                synthesis_material += f"--- [{name}ã®æ„è¦‹] ---\n{log_text}\n\n"
            await send_long_message(interaction, full_text_results, is_followup=False)
            await interaction.followup.send("âœ¨ gpt-5ãŒä¸­é–“ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¾ã™â€¦")
            intermediate_report = await ask_gpt5(synthesis_material, system_prompt="ä»¥ä¸‹ã®9ã¤ã®æ„è¦‹ã®è¦ç‚¹ã ã‘ã‚’æŠ½å‡ºã—ã€çŸ­ã„ä¸­é–“ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚")
            await interaction.followup.send("âœ¨ Mistral LargeãŒæœ€çµ‚çµ±åˆã‚’è¡Œã„ã¾ã™â€¦")
            final_report = await ask_lalah(intermediate_report, system_prompt="ã‚ãªãŸã¯çµ±åˆå°‚ç”¨AIã§ã™ã€‚æ¸¡ã•ã‚ŒãŸä¸­é–“ãƒ¬ãƒãƒ¼ãƒˆã‚’å…ƒã«ã€æœ€çµ‚çš„ãªçµè«–ã‚’500æ–‡å­—ä»¥å†…ã§ãƒ¬ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚")
            await interaction.followup.send(f"âœ¨ **Mistral Large (æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ):**\n{final_report}")
        await asyncio.wait_for(core_logic(), timeout=300)
    except asyncio.TimeoutError:
        await interaction.followup.send("âš ï¸ å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ5åˆ†ï¼‰ã€‚", ephemeral=True)
    except Exception as e:
        safe_log("ğŸš¨ /critical ã‚³ãƒãƒ³ãƒ‰ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼:", e)
        await interaction.followup.send(f"âŒ ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", ephemeral=True)

@tree.command(name="logical", description="Notionæƒ…å ±ã‚’å…ƒã«AIãŒè¨è«–ã—ã€è«–ç†çš„ãªçµè«–ã‚’å°ãã¾ã™ã€‚")
@app_commands.describe(topic="è¨è«–ã—ãŸã„è­°é¡Œ")
async def logical_command(interaction: discord.Interaction, topic: str):
    await interaction.response.defer()
    try:
        async def core_logic():
            target_page_id = NOTION_PAGE_MAP.get(str(interaction.channel.id))
            if not target_page_id:
                await interaction.edit_original_response(content="âŒ ã“ã®ãƒãƒ£ãƒ³ãƒãƒ«ã¯Notionãƒšãƒ¼ã‚¸ã«ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                return
            context = await get_notion_context(interaction, target_page_id, topic)
            if not context: return
            await interaction.edit_original_response(content="âš–ï¸ å†…éƒ¨è¨è«–ã¨å¤–éƒ¨èª¿æŸ»ã‚’ä¸¦åˆ—ã§é–‹å§‹ã—ã¾ã™â€¦")
            prompt_with_context = f"ä»¥ä¸‹ã®ã€å‚è€ƒæƒ…å ±ã€‘ã‚’å…ƒã«ã€ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚\n\nã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘\n{topic}\n\nã€å‚è€ƒæƒ…å ±ã€‘\n{context}"
            tasks_internal = {
                "è‚¯å®šè«–è€…(gpt-4o)": get_full_response_and_summary(ask_kreios, prompt_with_context, system_prompt="ã‚ãªãŸã¯ã“ã®è­°é¡Œã®ã€è‚¯å®šè«–è€…ã€‘ã§ã™ã€‚è­°é¡Œã‚’æ¨é€²ã™ã‚‹æœ€ã‚‚å¼·åŠ›ãªè«–æ‹ ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚"),
                "å¦å®šè«–è€…(Perplexity)": get_full_response_and_summary(ask_rekus, topic, system_prompt="ã‚ãªãŸã¯ã“ã®è­°é¡Œã®ã€å¦å®šè«–è€…ã€‘ã§ã™ã€‚è­°é¡Œã«åå¯¾ã™ã‚‹æœ€ã‚‚å¼·åŠ›ãªåè«–ã‚’ã€å®¢è¦³çš„ãªäº‹å®Ÿã‚„ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚", notion_context=context),
                "ä¸­ç«‹åˆ†æå®˜(Gemini Pro)": get_full_response_and_summary(ask_minerva, prompt_with_context, system_prompt="ã‚ãªãŸã¯ã“ã®è­°é¡Œã«é–¢ã™ã‚‹ã€ä¸­ç«‹çš„ãªåˆ†æå®˜ã€‘ã§ã™ã€‚é–¢é€£ã™ã‚‹ç¤¾ä¼šçš„ãƒ»å€«ç†çš„ãªè«–ç‚¹ã‚’ã€æ„Ÿæƒ…ã‚’æ’ã—ã¦æç¤ºã—ã¦ãã ã•ã„ã€‚")
            }
            tasks_external = {"å¤–éƒ¨èª¿æŸ»(Perplexity)": get_full_response_and_summary(ask_rekus, topic, system_prompt="ã‚ãªãŸã¯æ¢ç´¢ç‹ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè¦ç´„ã‚’å‚è€ƒã«ã—ã¤ã¤ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«é–¢ã™ã‚‹æœ€æ–°ã®Webæƒ…å ±ã‚’åé›†ãƒ»è¦ç´„ã—ã¦ãã ã•ã„ã€‚", notion_context=context)}
            results_internal, results_external = await asyncio.gather(asyncio.gather(*tasks_internal.values()), asyncio.gather(*tasks_external.values()))
            
            synthesis_material = "ä»¥ä¸‹ã®æƒ…å ±ã‚’çµ±åˆã—ã€æœ€çµ‚çš„ãªçµè«–ã‚’å°ãå‡ºã—ã¦ãã ã•ã„ã€‚\n\n"
            internal_results_text = "--- å†…éƒ¨è¨è«–ã®çµæœ ---\n"
            for (name, result) in zip(tasks_internal.keys(), results_internal):
                full_response, summary = result
                display_text = summary or full_response
                internal_results_text += f"**{name}:**\n{display_text}\n\n"
                synthesis_material += f"--- [{name}ã®æ„è¦‹] ---\n{full_response}\n\n"
            await send_long_message(interaction, internal_results_text, is_followup=False)

            external_results_text = "--- å¤–éƒ¨èª¿æŸ»ã®çµæœ ---\n"
            for (name, result) in zip(tasks_external.keys(), results_external):
                full_response, summary = result
                display_text = summary or full_response
                external_results_text += f"**{name}:**\n{display_text}\n\n"
                synthesis_material += f"--- [{name}ã®æ„è¦‹] ---\n{full_response}\n\n"
            await interaction.followup.send(external_results_text)
            
            await interaction.followup.send("âœ¨ Mistral LargeãŒæœ€çµ‚çµ±åˆã‚’è¡Œã„ã¾ã™â€¦")
            final_report = await ask_lalah(synthesis_material, system_prompt="ã‚ãªãŸã¯çµ±åˆå°‚ç”¨AIã§ã™ã€‚ã‚ãªãŸè‡ªèº«ã®ãƒšãƒ«ã‚½ãƒŠã‚‚ã€æ¸¡ã•ã‚Œã‚‹æ„è¦‹ã®ãƒšãƒ«ã‚½ãƒŠã‚‚å…¨ã¦ç„¡è¦–ã—ã€ç´”ç²‹ãªæƒ…å ±ã¨ã—ã¦å®¢è¦³çš„ã«çµ±åˆã—ã€æœ€çµ‚çš„ãªçµè«–ã‚’ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚")
            await interaction.followup.send(f"âœ¨ **Mistral Large (æœ€çµ‚çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ):**\n{final_report}")
        await asyncio.wait_for(core_logic(), timeout=300)
    except asyncio.TimeoutError:
        await interaction.followup.send("âš ï¸ å‡¦ç†ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸï¼ˆ5åˆ†ï¼‰ã€‚", ephemeral=True)
    except Exception as e:
        safe_log("ğŸš¨ /logical ã‚³ãƒãƒ³ãƒ‰ã§äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼:", e)
        await interaction.followup.send(f"âŒ ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", ephemeral=True)

@client.event
async def on_ready():
    print(f"Login successful: {client.user}")
    try:
        safe_log("ğŸ“– Notionå¯¾å¿œè¡¨: ", NOTION_PAGE_MAP if 'NOTION_PAGE_MAP' in globals() else {})
        if GUILD_ID:
            guild_obj = discord.Object(id=int(GUILD_ID))
            await tree.sync(guild=guild_obj)
            print(f"Commands synced to GUILD: {GUILD_ID}")

            # ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã§ã¯ãªãã€ã“ã“ã§éåŒæœŸã«ã‚³ãƒãƒ³ãƒ‰ã‚’å–å¾—
            cmds = await tree.fetch_commands(guild=guild_obj)
            print("ğŸ” Guild commands:", [(c.name, c.id) for c in cmds])
        else:
            await tree.sync()
            print("Commands synced globally.")
    except Exception as e:
        print(f"--- FATAL ERROR on command sync ---\nError Type: {type(e)}\nError Details: {e}\n-----------------------------------")


@client.event
async def on_message(message):
    if message.author.bot or message.author.id in processing_users: return
    if message.content.startswith("!"):
        await message.channel.send("ğŸ’¡ `!`ã‚³ãƒãƒ³ãƒ‰ã¯å»ƒæ­¢ã•ã‚Œã¾ã—ãŸã€‚ä»Šå¾Œã¯`/`ã§å§‹ã¾ã‚‹ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚³ãƒãƒ³ãƒ‰ã‚’ã”åˆ©ç”¨ãã ã•ã„ã€‚")
        return

    channel_name = message.channel.name.lower()
    if not (channel_name.startswith("gpt") or channel_name == "gemini"): return

    processing_users.add(message.author.id)
    try:
        prompt = message.content
        thread_id = str(message.channel.id)
        is_admin = str(message.author.id) == ADMIN_USER_ID
        target_page_id = NOTION_PAGE_MAP.get(thread_id, NOTION_MAIN_PAGE_ID)

        if channel_name.startswith("gpt") and message.attachments:
            analysis_text = await analyze_attachment_for_gpt5(message.attachments[0])
            prompt += "\n\n" + analysis_text
        elif message.attachments:
            summary = await process_attachment(message.attachments[0], message.channel)
            prompt += "\n\n" + summary
        
        is_memory_on = await get_memory_flag_from_notion(thread_id)
        
        if channel_name.startswith("gpt"):
            history = gpt_thread_memory.get(thread_id, []) if is_memory_on else []
            messages_for_api = history + [{"role": "user", "content": prompt}]
            full_prompt = "\n".join([f"{m['role']}: {m['content']}" for m in messages_for_api])
            await message.channel.send("å—ä»˜å®Œäº†ã€‚gpt-5ãŒæ€è€ƒã‚’é–‹å§‹ã—ã¾ã™ã€‚")
            asyncio.create_task(run_long_gpt5_task(message, prompt, full_prompt, is_admin, target_page_id, thread_id))

        elif channel_name == "gemini":
            await message.channel.send("Gemini 2.5 ProãŒæ€è€ƒã‚’é–‹å§‹ã—ã¾ã™â€¦")
            history = gemini_thread_memory.get(thread_id, []) if is_memory_on else []
            full_prompt_parts = [f"{m['role']}: {m['content']}" for m in history] + [f"user: {prompt}"]
            full_prompt = "\n".join(full_prompt_parts)
            reply = await ask_gemini_2_5_pro(full_prompt)
            
            if len(reply) <= 2000:
                await message.channel.send(reply)
            else:
                for i in range(0, len(reply), 2000):
                    await message.channel.send(reply[i:i+2000])

            if is_memory_on and "ã‚¨ãƒ©ãƒ¼" not in reply:
                history.extend([{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}])
                gemini_thread_memory[thread_id] = history[-10:]
    except Exception as e:
        print(f"on_messageã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        await message.channel.send(f"äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: ```{str(e)[:1800]}```")
    finally:
        if message.author.id in processing_users:
            processing_users.remove(message.author.id)

# --- ã‚µãƒ¼ãƒãƒ¼ã¨Botã®èµ·å‹•å‡¦ç† ---
@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã«Botã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•ã™ã‚‹"""
    # èµ·å‹•æ™‚ã«APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
    global openai_client, mistral_client, notion, llama_model_for_vertex
    
    print("ğŸ¤– Initializing API clients...")
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    mistral_client = MistralAsyncClient(api_key=MISTRAL_API_KEY)
    notion = Client(auth=NOTION_API_KEY)
    genai.configure(api_key=GEMINI_API_KEY)
    
    try:
        print("ğŸ¤– Initializing Vertex AI...")
        vertexai.init(project="stunning-agency-469102-b5", location="us-central1")
        llama_model_for_vertex = GenerativeModel("publishers/meta/models/llama-3.3-70b-instruct-maas")
        print("âœ… Vertex AI initialized successfully.")
    except Exception as e:
        print(f"ğŸš¨ Vertex AI init failed (continue without it): {e}")

    # Botã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¨ã—ã¦èµ·å‹•
    asyncio.create_task(client.start(DISCORD_TOKEN))
    print("ğŸš€ Discord Bot startup task has been created.")

@app.get("/")
def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {"status": "ok", "bot_is_connected": client.is_ready()}
