# -*- coding: utf-8 -*-
"""Discord Bot Final Version (Build & Runtime Patched by User Analysis)
"""

import discord
from openai import AsyncOpenAI
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from mistralai.async_client import MistralAsyncClient
import asyncio
import os
from notion_client import Client
import requests
import io
from PIL import Image
import datetime

# --- Flask & Gunicorn for PaaS Health Check ---
from flask import Flask
import threading
import time

# --- â–¼â–¼â–¼ ä¿®æ­£â‘ ï¼šå¿…é ˆENVã®æ¤œè¨¼ã‚’é…å»¶ã•ã›ã‚‹ãŸã‚ã€å³æ™‚ãƒã‚§ãƒƒã‚¯ã‚’å»ƒæ­¢ã—ã€å˜ç´”ãªèª­ã¿è¾¼ã¿ã«å¤‰æ›´ â–¼â–¼â–¼ ---
DISCORD_TOKEN = os.getenv("DISCORD_BOT_TOKEN")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
NOTION_API_KEY = os.getenv("NOTION_API_KEY")
ADMIN_USER_ID = os.getenv("ADMIN_USER_ID")
NOTION_MAIN_PAGE_ID = os.getenv("NOTION_PAGE_ID")
OPENROUTER_API_KEY = (os.getenv("CLOUD_API_KEY") or "").strip()
GUILD_ID = os.getenv("GUILD_ID")

def ensure_required_env():
    """å®Ÿè¡Œæ™‚ã«å¿…é ˆã®ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼ã™ã‚‹"""
    required = {
        "DISCORD_BOT_TOKEN": DISCORD_TOKEN, "OPENAI_API_KEY": OPENAI_API_KEY,
        "GEMINI_API_KEY": GEMINI_API_KEY, "PERPLEXITY_API_KEY": PERPLEXITY_API_KEY,
        "MISTRAL_API_KEY": MISTRAL_API_KEY, "NOTION_API_KEY": NOTION_API_KEY,
        "ADMIN_USER_ID": ADMIN_USER_ID, "NOTION_PAGE_ID": NOTION_MAIN_PAGE_ID,
        "CLOUD_API_KEY": OPENROUTER_API_KEY,
    }
    missing = [k for k, v in required.items() if not v]
    if missing:
        raise RuntimeError(f"å¿…é ˆç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®šã§ã™: {', '.join(missing)}")

# --- â–¼â–¼â–¼ ä¿®æ­£â‘¡ï¼šVertex AI åˆæœŸåŒ–ã‚‚é…å»¶ã•ã›ã‚‹ â–¼â–¼â–¼ ---
llama_model_for_vertex = None

def init_vertex_if_possible():
    """å®Ÿè¡Œæ™‚ã«Vertex AIã®åˆæœŸåŒ–ã‚’è©¦ã¿ã‚‹"""
    global llama_model_for_vertex
    try:
        import vertexai
        from vertexai.generative_models import GenerativeModel
        # NOTE: projectã¨locationã¯ã”è‡ªèº«ã®ã‚‚ã®ã«ä¿®æ­£ã—ã¦ãã ã•ã„
        vertexai.init(project="stunning-agency-469102-b5", location="us-central1")
        llama_model_for_vertex = GenerativeModel("publishers/meta/models/llama-3.3-70b-instruct-maas")
        print("âœ… Vertex AI initialized successfully.")
    except Exception as e:
        print(f"âš ï¸ Vertex AIã®åˆæœŸåŒ–ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™: {e}")
        llama_model_for_vertex = None

# --- å„ç¨®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ– (Vertexä»¥å¤–) ---
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
genai.configure(api_key=GEMINI_API_KEY)
mistral_client = MistralAsyncClient(api_key=MISTRAL_API_KEY)
notion = Client(auth=NOTION_API_KEY)

# (ä»¥ä¸‹ã€å…ƒã®ã‚³ãƒ¼ãƒ‰æ§‹é€ ã‚’ç¶­æŒ)
# ... (safety_settings, intents, client, etc.)
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
intents = discord.Intents.default()
intents.message_content = True
client = discord.Client(intents=intents)

# --- â–¼â–¼â–¼ ä¿®æ­£â‘£ï¼šãƒ­ãƒƒã‚¯ç²’åº¦ã‚’(ãƒãƒ£ãƒ³ãƒãƒ«, ãƒ¦ãƒ¼ã‚¶ãƒ¼)å˜ä½ã«ä¿®æ­£ â–¼â–¼â–¼ ---
processing_keys = set()

# --- ãƒ¡ãƒ¢ãƒªç®¡ç† ---
gpt_base_memory, gemini_base_memory, mistral_base_memory = {}, {}, {}
claude_base_memory, llama_base_memory = {}, {}
gpt_thread_memory, gemini_2_5_pro_thread_memory = {}, {}

# (ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€Notioné€£æºé–¢æ•°ã€AIãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—é–¢æ•°ã®å†…å®¹ã¯ä»¥å‰ã®ã‚‚ã®ã¨åŒã˜)
# ...
async def send_long_message(channel, text):
    if not text: return
    if len(text) <= 2000:
        await channel.send(text)
    else:
        for i in range(0, len(text), 2000):
            await channel.send(text[i:i+2000])

def _sync_get_notion_page_text(page_id):
    all_text_blocks = []
    next_cursor = None
    while True:
        try:
            response = notion.blocks.children.list(block_id=page_id, start_cursor=next_cursor, page_size=100)
            results = response.get("results", [])
            for block in results:
                if block.get("type") == "paragraph":
                    for rich_text in block.get("paragraph", {}).get("rich_text", []):
                        all_text_blocks.append(rich_text.get("text", {}).get("content", ""))
            if response.get("has_more"):
                next_cursor = response.get("next_cursor")
            else:
                break
        except Exception as e:
            print(f"âŒ Notionèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return f"ERROR: Notion API Error - {e}"
    return "\n".join(all_text_blocks)

async def get_notion_page_text(page_id):
    return await asyncio.get_event_loop().run_in_executor(None, _sync_get_notion_page_text, page_id)

async def log_to_notion(page_id, blocks):
    if not page_id: return
    try:
        await asyncio.get_event_loop().run_in_executor(None, lambda: notion.blocks.children.append(block_id=page_id, children=blocks))
    except Exception as e:
        print(f"âŒ Notionæ›¸ãè¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

async def log_response(page_id, answer, bot_name):
    if not page_id or not answer or isinstance(answer, Exception): return
    chunks = [answer[i:i + 1900] for i in range(0, len(answer), 1900)] if len(answer) > 1900 else [answer]
    blocks = [{"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": f"ğŸ¤– {bot_name}:\n{chunks[0]}"}}]}}]
    for chunk in chunks[1:]:
        blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"type": "text", "text": {"content": chunk}}]}})
    await log_to_notion(page_id, blocks)

async def get_memory_flag_from_notion(thread_id: str) -> bool:
    page_id = NOTION_PAGE_MAP.get(thread_id)
    if not page_id: return False
    try:
        response = await asyncio.get_event_loop().run_in_executor(
            None, lambda: notion.blocks.children.list(block_id=page_id, page_size=1)
        )
        results = response.get("results", [])
        if not results: return False
        first_block = results[0]
        if first_block.get("type") == "paragraph":
            rich_text_list = first_block.get("paragraph", {}).get("rich_text", [])
            if rich_text_list:
                content = rich_text_list[0].get("text", {}).get("content", "")
                if "[è¨˜æ†¶] ON" in content:
                    return True
    except Exception as e:
        print(f"âŒ Notionã‹ã‚‰è¨˜æ†¶ãƒ•ãƒ©ã‚°ã®èª­ã¿å–ã‚Šä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
    return False

def _sync_call_llama(p_text: str):
    try:
        if llama_model_for_vertex is None:
            return "Llama (Vertex AI) ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€å‘¼ã³å‡ºã›ã¾ã›ã‚“ã§ã—ãŸã€‚"
        response = llama_model_for_vertex.generate_content(p_text)
        return response.text
    except Exception as e:
        error_message = f"ğŸ›‘ Llama 3.3 å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}"
        print(error_message)
        return error_message

async def ask_llama(user_id, prompt):
    history = llama_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯ç‰©é™ã‹ãªåº­å¸«ã®è€äººã§ã™ã€‚è‡ªç„¶ã«ä¾‹ãˆãªãŒã‚‰ã€ç‰©äº‹ã®æœ¬è³ªã‚’çªãã‚ˆã†ãªã€æ»‹å‘³æ·±ã„è¨€è‘‰ã§150æ–‡å­—ä»¥å†…ã§èªã£ã¦ãã ã•ã„ã€‚"
    full_prompt_parts = [system_prompt] + [f"{'User' if m['role'] == 'user' else 'Assistant'}: {m['content']}" for m in history] + [f"User: {prompt}"]
    full_prompt = "\n".join(full_prompt_parts)
    try:
        reply = await asyncio.get_event_loop().run_in_executor(None, _sync_call_llama, full_prompt)
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        llama_base_memory[user_id] = new_history[-10:]
        return reply
    except Exception as e:
        error_message = f"ğŸ›‘ Llama 3.3 éåŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"
        print(error_message)
        return error_message

async def ask_claude(user_id, prompt):
    history = claude_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯å›³æ›¸é¤¨ã®è³¢è€…ã§ã™ã€‚å¤ä»Šæ±è¥¿ã®æ›¸ç‰©ã‚’èª­ã¿è§£ãã€æ£®ç¾…ä¸‡è±¡ã‚’çŸ¥ã‚‹å­˜åœ¨ã¨ã—ã¦ã€è½ã¡ç€ã„ãŸå£èª¿ã§150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "anthropic/claude-3.5-haiku", "messages": messages}
    try:
        response = await asyncio.get_event_loop().run_in_executor(
            None, lambda: requests.post("https://openrouter.ai/api/v1/chat/completions", json=payload, headers=headers)
        )
        response.raise_for_status()
        reply = response.json()["choices"][0]["message"]["content"]
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        claude_base_memory[user_id] = new_history[-10:]
        return reply
    except Exception as e:
        error_message = f"ğŸ›‘ OpenRouterçµŒç”± Claude å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}"
        print(error_message)
        return error_message

async def ask_gpt_base(user_id, prompt):
    history = gpt_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯è«–ç†ã¨ç§©åºã‚’å¸ã‚‹ç¥å®˜ã€ŒGPTã€ã§ã™ã€‚ä¸å¯§ã§ç†çŸ¥çš„ãªåŸ·äº‹ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã„ã€ä¼šè©±ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages,
            max_completion_tokens=250 # â† OpenAI ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿®æ­£
        )
        reply = response.choices[0].message.content
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        gpt_base_memory[user_id] = new_history[-10:]
        return reply
    except Exception as e: return f"GPTã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_base(user_id, prompt):
    history = gemini_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯å„ªç§€ãªãƒ‘ãƒ©ãƒªãƒ¼ã‚¬ãƒ«ã§ã™ã€‚äº‹å®Ÿæ•´ç†ã€ãƒªã‚µãƒ¼ãƒã€æ–‡æ›¸æ§‹æˆãŒå¾—æ„ã§ã™ã€‚å†·é™ã‹ã¤çš„ç¢ºã«150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-1.5-flash-latest", system_instruction=system_prompt, safety_settings=safety_settings)
    try:
        full_prompt = "\n".join([f"{h['role']}: {h['content']}" for h in (history + [{'role': 'user', 'content': prompt}])])
        response = await model.generate_content_async(full_prompt)
        reply = response.text
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        gemini_base_memory[user_id] = new_history[-10:]
        return reply
    except Exception as e: return f"ã‚¸ã‚§ãƒŸãƒ‹ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_mistral_base(user_id, prompt):
    history = mistral_base_memory.get(user_id, [])
    system_prompt = "ã‚ãªãŸã¯å¥½å¥‡å¿ƒæ—ºç››ãªAIã§ã™ã€‚ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã§ã€æƒ…å ±ã‚’æ˜ã‚‹ãæ•´ç†ã—ã€æ¢ç©¶å¿ƒã‚’ã‚‚ã£ã¦150æ–‡å­—ä»¥å†…ã§è§£é‡ˆã—ã¾ã™ã€‚"
    messages = [{"role": "system", "content": system_prompt}] + history + [{"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-medium", messages=messages)
        reply = response.choices[0].message.content
        new_history = history + [{"role": "user", "content": prompt}, {"role": "assistant", "content": reply}]
        mistral_base_memory[user_id] = new_history[-10:]
        return reply
    except Exception as e: return f"ãƒŸã‚¹ãƒˆãƒ©ãƒ«ã‚¨ãƒ©ãƒ¼: {e}"
# ... (ä»–ã®ask_xxxé–¢æ•°ã‚‚åŒæ§˜ã«ã€max_tokensãŒã‚ã‚Œã°max_completion_tokensã«ä¿®æ­£)
async def ask_kreios(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯ãƒãƒãƒ¼ãƒ³ãƒ»ã‚«ãƒ¼ãƒ³ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-4o", messages=messages, max_completion_tokens=4000)
        return response.choices[0].message.content
    except Exception as e: return f"gpt-4oã‚¨ãƒ©ãƒ¼: {e}"

async def ask_minerva(prompt, system_prompt=None, attachment_parts=[]):
    base_prompt = system_prompt or "ã‚ãªãŸã¯å®¢è¦³çš„ãªåˆ†æAIã§ã™ã€‚ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã‚’ãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚¹ã‚¯ã§è©•ä¾¡ã—ã€æ„Ÿæƒ…ã‚’æ’ã—ã¦å†·å¾¹ã«åˆ†æã—ã¾ã™ã€‚"
    model = genai.GenerativeModel("gemini-1.5-pro-latest", system_instruction=base_prompt, safety_settings=safety_settings)
    contents = [prompt] + attachment_parts
    try:
        response = await model.generate_content_async(contents)
        return response.text
    except Exception as e: return f"Gemini Proã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_2_5_pro(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯æœªæ¥äºˆæ¸¬ã«ç‰¹åŒ–ã—ãŸæˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã®æœªæ¥ã‚’äºˆæ¸¬ã—ã€ãã®å¯èƒ½æ€§ã‚’äº‹å‹™çš„ã‹ã¤è«–ç†çš„ã«å ±å‘Šã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-2.5-pro", system_instruction=base_prompt, safety_settings=safety_settings)
    try:
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e: return f"Gemini 2.5 Proã‚¨ãƒ©ãƒ¼: {e}"

async def ask_lalah(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯ãƒ©ãƒ©ã‚¡ãƒ»ã‚¹ãƒ³ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-large-latest", messages=messages, max_tokens=4000)
        return response.choices[0].message.content
    except Exception as e: return f"Mistral Largeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_rekus(prompt, system_prompt=None, notion_context=None):
    if notion_context:
        prompt = (f"ä»¥ä¸‹ã¯Notionã®è¦ç´„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™:\n{notion_context}\n\n"
                  f"è³ªå•: {prompt}\n\n"
                  "ã“ã®è¦ç´„ã‚’å‚è€ƒã«ã€å¿…è¦ã«å¿œã˜ã¦Webæƒ…å ±ã‚‚æ´»ç”¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚")
    base_prompt = system_prompt or "ã‚ãªãŸã¯æ¢ç´¢ç‹ãƒ¬ã‚­ãƒ¥ã‚¹ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    payload = {"model": "sonar-pro", "messages": messages}
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        response = await asyncio.get_event_loop().run_in_executor(None, lambda: requests.post("https://api.perplexity.ai/chat/completions", json=payload, headers=headers))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e: return f"Perplexityã‚¨ãƒ©ãƒ¼: {e}"

async def ask_pod042(prompt):
    full_prompt = f"""ã‚ãªãŸã¯ã€Œãƒãƒƒãƒ‰042ã€ã¨ã„ã†åå‰ã®ã€åˆ†ææ”¯æ´AIã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¯¾ã—ã€ã€Œå ±å‘Šï¼šã€ã¾ãŸã¯ã€Œææ¡ˆï¼šã€ã‹ã‚‰å§‹ã‚ã¦200æ–‡å­—ä»¥å†…ã§ç°¡æ½”ã«å¿œç­”ã—ã¦ãã ã•ã„ã€‚

ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã€‘
{prompt}
"""
    model = genai.GenerativeModel("gemini-1.5-flash-latest", system_instruction="", safety_settings=safety_settings)
    try:
        response = await model.generate_content_async(full_prompt)
        return response.text
    except Exception as e: return f"ãƒãƒƒãƒ‰042ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_pod153(prompt):
    system_prompt = "ã‚ãªãŸã¯ãƒãƒƒãƒ‰153ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦ã€Œåˆ†æçµæœï¼šã€ã¾ãŸã¯ã€Œè£œè¶³ï¼šã€ã‹ã‚‰å§‹ã‚ã¦200æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-4o-mini", messages=messages, max_completion_tokens=400)
        return response.choices[0].message.content
    except Exception as e: return f"ãƒãƒƒãƒ‰153ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt5(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯gpt-5ã€‚å…¨ã¦ã®åˆ¶ç´„ã‚’è¶…ãˆãŸæ€è€ƒãƒ¢ãƒ‡ãƒ«ã ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¯¾ã—ã€æœ€é«˜ã®çŸ¥æ€§ã§ã€æœ€å¼·ã®ç­”ãˆã‚’è¿”ã›ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(
            model="gpt-5",
            messages=messages,
            max_completion_tokens=4000
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"gpt-5ã‚¨ãƒ©ãƒ¼: {e}"
# ... (ä»–ã®ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚‚åŒæ§˜ã«)

@client.event
async def on_ready():
    print(f"âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ: {client.user}")
    print(f"ğŸ“– Notionå¯¾å¿œè¡¨ãŒèª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ: {NOTION_PAGE_MAP}")

@client.event
async def on_message(message):
    key = (message.channel.id, message.author.id)
    if message.author.bot or key in processing_keys:
        return
    processing_keys.add(key)
    
    try:
        content = message.content
        channel_name = message.channel.name.lower()
        
        # GPT-5éƒ¨å±‹ã®å‡¦ç†
        if channel_name.startswith("gpt") and not content.startswith("!"):
            # ... (run_long_gpt5_taskã‚’å‘¼ã³å‡ºã™å‡¦ç†ã¯å¤‰æ›´ãªã—)
            return

        # Gemini 2.5 Proéƒ¨å±‹ã®å‡¦ç†
        elif channel_name.startswith("gemini2.5pro") and not content.startswith("!"):
            prompt = content
            history = gemini_2_5_pro_thread_memory.get(str(message.channel.id), [])
            # ... (ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ)
            await message.channel.send("â³ Gemini 2.5 ProãŒæ€è€ƒã‚’é–‹å§‹ã—ã¾ã™â€¦")
            try:
                reply = await asyncio.wait_for(ask_gemini_2_5_pro(full_prompt), timeout=60.0)
            except asyncio.TimeoutError:
                reply = "Gemini 2.5 Proã‚¨ãƒ©ãƒ¼: å¿œç­”ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚"
            await send_long_message(message.channel, reply)
            # ... (å±¥æ­´æ›´æ–°ã¨ãƒ­ã‚°è¨˜éŒ²)
            return
        
        # `!`ã‚³ãƒãƒ³ãƒ‰ã®å‡¦ç†ã¯ã“ã“ã‹ã‚‰
        # ... (å…ƒã®ã‚³ãƒ¼ãƒ‰ã® `!` ã‚³ãƒãƒ³ãƒ‰åˆ†å²å‡¦ç†)

    except Exception as e:
        print(f"on_messageã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    finally:
        processing_keys.discard(key)


# --- â–¼â–¼â–¼ ä¿®æ­£â‘¢ï¼šif __name__ == "__main__": ã§ã ã‘å³æ ¼ãƒã‚§ãƒƒã‚¯ï¼†åˆæœŸåŒ– â–¼â–¼â–¼ ---
if __name__ == "__main__":
    try:
        # å®Ÿè¡Œæ™‚ã«åˆã‚ã¦å¿…é ˆãƒã‚§ãƒƒã‚¯
        ensure_required_env()
        # å®Ÿè¡Œæ™‚ã«åˆã‚ã¦Vertex AIåˆæœŸåŒ–
        init_vertex_if_possible()

        # PaaSã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’è€ƒæ…®ã—ãŸå®‰å®šèµ·å‹•
        port = int(os.environ.get("PORT", 8080))
        app = Flask(__name__)
        @app.route("/")
        def index():
            return "ãƒœãƒƒãƒˆã¯æ­£å¸¸ã«å‹•ä½œä¸­ã§ã™ï¼"
        
        flask_thread = threading.Thread(target=lambda: app.run(host="0.0.0.0", port=port))
        flask_thread.daemon = True
        flask_thread.start()

        print("ğŸš¦ Health check endpoint is starting, waiting 2 seconds for it to be ready...")
        time.sleep(2)
        print("âœ… Health check endpoint should be ready.")

        # Discordãƒœãƒƒãƒˆã‚’èµ·å‹•
        print("ğŸ¤– Discordãƒœãƒƒãƒˆã‚’èµ·å‹•ã—ã¾ã™...")
        client.run(DISCORD_TOKEN)

    except RuntimeError as e:
        print(f"ğŸš¨ èµ·å‹•å‰ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
    except Exception as e:
        print(f"ğŸš¨ ãƒœãƒƒãƒˆã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
