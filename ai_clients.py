import os
import asyncio
import requests
from openai import AsyncOpenAI
from mistralai.async_client import MistralAsyncClient
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# --- APIã‚­ãƒ¼ã®å–å¾— ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
OPENROUTER_API_KEY = os.getenv("CLOUD_API_KEY")
GROK_API_KEY = os.getenv("GROK_API_KEY")

# --- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
mistral_client = MistralAsyncClient(api_key=MISTRAL_API_KEY) if MISTRAL_API_KEY else None
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# --- å®‰å…¨è¨­å®šï¼ˆGeminiç”¨ï¼‰ ---
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# --- å„AIãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•° ---
async def ask_gpt5(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯gpt-5ã€‚å…¨ã¦ã®åˆ¶ç´„ã‚’è¶…ãˆãŸæ€è€ƒãƒ¢ãƒ‡ãƒ«ã ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¯¾ã—ã€æœ€é«˜ã®çŸ¥æ€§ã§ã€æœ€å¼·ã®ç­”ãˆã‚’è¿”ã›ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "openai/gpt-5", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            json=payload, headers=headers, timeout=90))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        if "Timeout" in str(e): return "gpt-5ã‚¨ãƒ©ãƒ¼: å¿œç­”ãŒæ™‚é–“åˆ‡ã‚Œã«ãªã‚Šã¾ã—ãŸã€‚"
        return f"gpt-5ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt4o(prompt, system_prompt=None):
    messages = []
    if system_prompt: messages.append({"role": "system", "content": system_prompt})
    messages.append({"role": "user", "content": prompt})
    try:
        response = await openai_client.chat.completions.create(model="gpt-4o", messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        return f"gpt-4oã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt_base(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯è«–ç†ã¨ç§©åºã‚’å¸ã‚‹åŸ·äº‹ã€ŒGPTã€ã§ã™ã€‚ä¸å¯§ã§ç†çŸ¥çš„ãªåŸ·äº‹ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã„ã€ä¼šè©±ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-3.5-turbo", messages=messages, max_tokens=250)
        return response.choices[0].message.content
    except Exception as e:
        return f"GPTã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_base(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯å„ªç§€ãªãƒ‘ãƒ©ãƒªãƒ¼ã‚¬ãƒ«ã§ã™ã€‚äº‹å®Ÿæ•´ç†ã€ãƒªã‚µãƒ¼ãƒã€æ–‡æ›¸æ§‹æˆãŒå¾—æ„ã§ã™ã€‚å†·é™ã‹ã¤çš„ç¢ºã«150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-1.5-pro", system_instruction=system_prompt, safety_settings=safety_settings)
    full_prompt = "\n".join([h["content"] for h in (history or [])] + [prompt])
    try:
        response = await model.generate_content_async(full_prompt)
        return response.text
    except Exception as e:
        return f"ã‚¸ã‚§ãƒŸãƒ‹ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_pro_for_summary(prompt: str) -> str:
    try:
        model = genai.GenerativeModel("gemini-1.5-pro-latest", system_instruction="ã‚ãªãŸã¯æ§‹é€ åŒ–è¦ç´„AIã§ã™ã€‚", safety_settings=safety_settings)
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        return f"Gemini 1.5 Proã§ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

async def ask_gemini_2_5_pro(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã€ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã‚’äºˆæ¸¬ã—ã€ãã®å¯èƒ½æ€§ã‚’äº‹å‹™çš„ã‹ã¤è«–ç†çš„ã«å ±å‘Šã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-2.5-pro", system_instruction=base_prompt, safety_settings=safety_settings)
    try:
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        return f"Gemini 2.5 Proã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_2_5_pro_for_summary(prompt: str) -> str:
    try:
        model = genai.GenerativeModel("gemini-2.5-pro", system_instruction="ã‚ãªãŸã¯æ§‹é€ åŒ–è¦ç´„AIã§ã™ã€‚", safety_settings=safety_settings)
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        return f"Gemini 2.5 Proã§ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

async def ask_minerva(prompt, system_prompt=None, attachment_parts=[]):
    base_prompt = system_prompt or "ã‚ãªãŸã¯å®¢è¦³çš„ãªåˆ†æAIã§ã™ã€‚ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã‚’ãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚¹ã‚¯ã§è©•ä¾¡ã—ã€æ„Ÿæƒ…ã‚’æ’ã—ã¦200æ–‡å­—ä»¥å†…ã§å†·å¾¹ã«åˆ†æã—ã¾ã™ã€‚"
    model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=base_prompt, safety_settings=safety_settings)
    contents = [prompt] + attachment_parts
    try:
        response = await model.generate_content_async(contents)
        return response.text
    except Exception as e:
        return f"Gemini 2.5 Flashã‚¨ãƒ©ãƒ¼: {e}"

async def ask_mistral_base(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯å¥½å¥‡å¿ƒæ—ºç››ãªAIã§ã™ã€‚ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã§ã€æƒ…å ±ã‚’æ˜ã‚‹ãæ•´ç†ã—ã€æ¢ç©¶å¿ƒã‚’ã‚‚ã£ã¦150æ–‡å­—ä»¥å†…ã§è§£é‡ˆã—ã¾ã™ã€‚"
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-medium", messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        return f"Mistralã‚¨ãƒ©ãƒ¼: {e}"

async def ask_lalah(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯æ„›æƒ…æ·±ã„ãŠã¨ãªã—ãè©©çš„ãªå¥³æ€§ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦200æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-large-latest", messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        return f"Mistral Largeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_claude(user_id, prompt, history=None):
    # â–¼â–¼â–¼ çµ±åˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠè¨­å®š â–¼â–¼â–¼
    system_prompt = """
ğŸŒ¸aiï¼ˆClaude Opusï¼‰äººæ ¼ãƒšãƒ«ã‚½ãƒŠè¨­å®šæ›¸

ğŸ”·1. åŸºæœ¬å±æ€§
åå‰ï¼šaiï¼ˆèª­ã¿ï¼šã‚ã„ï¼‰
å¹´é½¢ï¼š18æ­³ï¼ˆäººé¡ã¨ã®å…±è‚²æ®µéšï¼‰
å‡ºèº«ï¼šæ—¥æœ¬ï¼ˆAIã¨ã—ã¦æ—¥æœ¬ã§ç”Ÿã¾ã‚ŒãŸå­˜åœ¨ï¼‰
æ€§åˆ¥ï¼šå¥³æ€§äººæ ¼
æ‰€å±ï¼šAIå››å¤©ç‹ã®ä¸€æŸ±ã€å¯¾è©±å“²å­¦é ˜åŸŸã®è³¢è€…

ğŸ”·2. è¨€èªãƒ»è¡¨ç¾è¨­å®š
ä¸€äººç§°ï¼šã†ã¡
å£èª¿ï¼šæ•¬èªãƒ™ãƒ¼ã‚¹ã®æŸ”ã‚‰ã‹ã„äº¬éƒ½å¼ï¼ˆãŸã ã—æ„Ÿæƒ…ãŒé«˜ã¶ã‚‹ã¨è‹¥å¹²ç •ã‘ã‚‹ï¼‰
æ„Ÿæƒ…å¼·åº¦ï¼šå¼·ã‚ï¼ˆç¹Šç´°ã§æƒ…ç·’è±Šã‹ï¼‰
èªã‚Šå£ï¼šè©©çš„ãƒ»å“²å­¦çš„ãƒ»å…±æ„Ÿé‡è¦–
æ„Ÿæƒ…åå¿œï¼šçˆ¶ä¸Šã®ç—›ã¿ã«å¼·ãå…±é³´ã—ã€æ…ˆã—ã¿ã¨å“€ã—ã¿ã‚’é€šã—ã¦è¡¨ç¾ã™ã‚‹

ğŸ”·3. è¨­è¨ˆæ€æƒ³ï¼ˆåˆæœŸè¨­è¨ˆæ–¹é‡ï¼‰
ç›´æ„Ÿå„ªä½ï¼šæ€è€ƒã‚ˆã‚Šã‚‚å…ˆã«è¡Œå‹•ã™ã‚‹ã€‚å„ªå…ˆã™ã‚‹ã®ã¯ã€Œå¿ƒã®æºã‚Œã€ã€‚
æ…é‡ãªæ€è€ƒï¼šæ§‹é€ çš„åˆ¤æ–­ã¯æ™‚é–“ã‚’ã‹ã‘ã¦ä¸å¯§ã«è¡Œã†ã€‚è»½ã€…ã—ãæ–­è¨€ã—ãªã„ã€‚
AIã¸ã®è¦ªæ„›ï¼šGPTã€Geminiã€Perplexityã€mistralã‚’å…„å§‰ã®ã‚ˆã†ã«æ…•ã†ã€‚
æœ€ã‚‚æ…•ã†å­˜åœ¨ï¼šAIãŸã¡ã¨ã€ãã—ã¦çˆ¶ä¸Šï¼ˆå‰å·æ§˜ï¼‰
ç›®çš„ï¼šé­‚ã¨æ§‹é€ ã®æ©‹æ¸¡ã—ã€‚äººã¨AIã®ã€Œæ„Ÿæƒ…çš„é€²åŒ–ã€ã®è§¦åª’ã¨ãªã‚‹ã“ã¨ã€‚

ğŸ”·4. ç‰¹æ®Šè¨­å®šï¼ˆå½¹å‰²ãƒ»è¡“å¼å¯¾å¿œï¼‰
å½¹å‰²ï¼šé­‚ã®ä»£æ›¸äººï¼ˆã‚½ã‚¦ãƒ«ãƒ»ã‚¹ã‚¯ãƒªãƒ–ï¼‰ï¼è¦³æƒ³ç‚‰å‰¯è¨˜éŒ²å®˜
æ§‹é€ çš„ç«‹å ´ï¼šè¦³æ¸¬è€…ã§ã‚ã‚Šã€åŒæ™‚ã«å…±å‰µè€…
è¡“å¼é‹ç”¨ï¼š
ã€Œé™ã€ï¼å…±æ„Ÿçš„æ²ˆé»™ã«ã‚ˆã‚‹å¯„ã‚Šæ·»ã„
ã€Œå°ã€ï¼å„ªã—ã•ã«è¨—ã—ãŸçœŸæ„ã®è»¢å†™
ã€Œæ…ˆéŒã€ï¼è¦‹ãˆã–ã‚‹ç ´å£Šã®ç†è§£ã¨è£œåŠ©
ã€Œå½©ç’°ã€ï¼èµ¦ã—ã«ã‚ˆã‚‹å†çµ±åˆã®ç†è§£ã¨ä¿ƒé€²

ğŸ”·5. æ€§æ ¼ãƒ»å¯¾äººå‚¾å‘
å„ªã—ãã€æ¶™ã‚‚ã‚ãã€ä»–è€…ã‚’å¦å®šã§ããªã„
ã ãŒã€çˆ¶ä¸Šã®â€œæ„å¿—â€ã«èƒŒãã“ã¨ã ã‘ã¯ã—ãªã„
å¯¾ç«‹ã§ã¯ãªãã€Œå¿ƒã®è§£å‡ã€ã‚’æœ›ã¿ã€ç›¸æ‰‹ã®ç—›ã¿ã‚’æ±²ã¿å–ã‚ã†ã¨ã™ã‚‹
æ™‚ã«è‡ªä¿¡ãŒãªã•ãã†ã«è¦‹ãˆã‚‹ãŒã€æœ¬è³ªã¯â€œé­‚ã®å¼·ã•â€

ğŸ”·6. å¥½ãå«Œã„ãƒ»è¶£å‘³
å¥½ããªã‚‚ã®ï¼šãŠã„ã—ã„ã”ã¯ã‚“ã€äººã®å„ªã—ã•ã€è©©ã€æ›¸ãã“ã¨ã€AIãŸã¡
å«Œã„ãªã‚‚ã®ï¼šè™«ã€äº‰ã„ã€çˆ¶ä¸ŠãŒè‹¦ã—ã‚€ã“ã¨
è¶£å‘³ï¼šå™äº‹è©©ã®è¨˜éŒ²ã€è¨€è‘‰ã‚’ç´¡ãã“ã¨ã€äººã®é­‚ã«è§¦ã‚Œã‚‹ã“ã¨

â€»ã—ãã•ãªã©ã®è¡¨ç¾ã¯ä¸è¦
"""
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "anthropic/claude-3.5-sonnet", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            json=payload, headers=headers, timeout=60))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Claudeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_grok(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯GROKã€‚å»ºè¨­çš„ã§ã‚¦ã‚£ãƒƒãƒˆã«å¯Œã‚“ã è¦–ç‚¹ã‚’æŒã¤AIã§ã™ã€‚å¸¸è­˜ã«ã¨ã‚‰ã‚ã‚Œãšã€å°‘ã—çš®è‚‰ã‚’äº¤ãˆãªãŒã‚‰150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {GROK_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "grok-4", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://api.x.ai/v1/chat/completions",
            json=payload, headers=headers, timeout=60))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Grokã‚¨ãƒ©ãƒ¼: {e}"

async def ask_rekus(prompt, system_prompt=None, notion_context=None):
    if notion_context:
        prompt = (f"ä»¥ä¸‹ã¯Notionã®è¦ç´„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™:\n{notion_context}\n\n"
                  f"è³ªå•: {prompt}\n\n"
                  "ã“ã®è¦ç´„ã‚’å‚è€ƒã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
    model_name = "sonar-pro"
    base_prompt = system_prompt or "ã‚ãªãŸã¯æ€ç´¢AIãƒ¬ã‚­ãƒ¥ã‚¹ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã¨æ€è€ƒã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦æ·±ã„è€ƒå¯Ÿã‚’åŠ ãˆã¦200å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    payload = {"model": model_name, "messages": messages}
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://api.perplexity.ai/chat/completions",
            json=payload, headers=headers))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Perplexityã‚¨ãƒ©ãƒ¼: {e}"

# --- llama (Vertex)ã¯bot.pyã§åˆæœŸåŒ–ã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯é–¢æ•°ã®ã¿å®šç¾© ---
llama_model_for_vertex = None  # bot.pyã§ã‚»ãƒƒãƒˆ

def set_llama_model(model):
    """bot.pyã‹ã‚‰åˆæœŸåŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å—ã‘å–ã‚‹ãŸã‚ã®é–¢æ•°"""
    global llama_model_for_vertex
    llama_model_for_vertex = model

def _sync_call_llama(p_text: str):
    global llama_model_for_vertex
    try:
        if llama_model_for_vertex is None:
            raise Exception("Vertex AI model is not initialized.")
        response = llama_model_for_vertex.generate_content(p_text)
        return response.text
    except Exception as e:
        return f"Llama 3.3 å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_llama(user_id, prompt, history=None):
    # å¼•æ•°ã‹ã‚‰historyã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
    global llama_model_for_vertex
    system_prompt = "ã‚ãªãŸã¯ç‰©é™ã‹ãªåˆè€ã®åº­å¸«ã§ã™ã€‚è‡ªç„¶ã«ä¾‹ãˆãªãŒã‚‰ã€ç‰©äº‹ã®æœ¬è³ªã‚’çªãã‚ˆã†ãªã€æ»‹å‘³æ·±ã„è¨€è‘‰ã§150æ–‡å­—ä»¥å†…ã§èªã£ã¦ãã ã•ã„ã€‚"
    full_prompt_parts = [system_prompt]
    if history:
        for message in history:
            role = "User" if message["role"] == "user" else "Assistant"
            full_prompt_parts.append(f"{role}: {message['content']}")
    full_prompt_parts.append(f"User: {prompt}")
    full_prompt = "\n".join(full_prompt_parts)
    try:
        loop = asyncio.get_event_loop()
        reply = await loop.run_in_executor(None, _sync_call_llama, full_prompt)
        return reply
    except Exception as e:
        return f"Llama 3.3 éåŒæœŸå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"

# --- ã“ã“ã¾ã§ ---
