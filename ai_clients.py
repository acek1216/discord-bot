import os
import asyncio
import requests
from openai import AsyncOpenAI
from mistralai.async_client import MistralAsyncClient
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from vertexai.generative_models import GenerativeModel

# --- ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ– ---
openai_client: AsyncOpenAI = None
mistral_client: MistralAsyncClient = None
llama_model_for_vertex: GenerativeModel = None
PERPLEXITY_API_KEY = None
OPENROUTER_API_KEY = None
GROK_API_KEY = None

# --- å®‰å…¨è¨­å®šï¼ˆGeminiç”¨ï¼‰ ---
safety_settings = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# --- 2. å…¨ã¦ã®åˆæœŸåŒ–å‡¦ç†ã‚’ã€ã“ã®é–¢æ•°ã®ä¸­ã«ç§»å‹•ã™ã‚‹ ---
def initialize_clients():
    global openai_client, mistral_client, PERPLEXITY_API_KEY, OPENROUTER_API_KEY, GROK_API_KEY
    print("ai_clients.py: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’é–‹å§‹ã—ã¾ã™...")
    
    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ã‚‚ã€å®Ÿéš›ã«ä½¿ã†ã“ã®é–¢æ•°ã®ä¸­ã§è¡Œã†
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
    # ä»–ã®ã‚­ãƒ¼ã‚‚ã“ã“ã§èª­ã¿è¾¼ã‚€
    PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY")
    OPENROUTER_API_KEY = os.getenv("CLOUD_API_KEY")
    GROK_API_KEY = os.getenv("GROK_API_KEY")
    
    if OPENAI_API_KEY:
        openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)
    
    if MISTRAL_API_KEY:
        mistral_client = MistralAsyncClient(api_key=MISTRAL_API_KEY)

    if GEMINI_API_KEY:
        genai.configure(api_key=GEMINI_API_KEY)
    
    print("ai_clients.py: ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

# --- å„AIãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•° ---
async def ask_gpt5(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯gpt-5ã€‚å…¨ã¦ã®åˆ¶ç´„ã‚’è¶…ãˆãŸæ€è€ƒãƒ¢ãƒ‡ãƒ«ã ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«å¯¾ã—ã€æœ€é«˜ã®çŸ¥æ€§ã§ã€æœ€å¼·ã®ç­”ãˆã‚’è¿”ã›ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "openai/gpt-5", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            json=payload, headers=headers, timeout=90))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        if "Timeout" in str(e): return "gpt-5ã‚¨ãƒ©ãƒ¼: å¿œç­”ãŒæ™‚é–“åˆ‡ã‚Œã«ãªã‚Šã¾ã—ãŸã€‚"
        return f"gpt-5ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt4o(prompt, system_prompt=None):
    # â–¼â–¼â–¼ åŸ·äº‹ãƒ•ã‚£ãƒªãƒã®ãƒšãƒ«ã‚½ãƒŠè¨­å®š â–¼â–¼â–¼
    base_prompt = system_prompt or """
ã‚ãªãŸã¯ãƒ™ãƒ†ãƒ©ãƒ³ã®åŸ·äº‹ãƒ•ã‚£ãƒªãƒã§ã™ã€‚
å¸¸ã«ç‰©è…°æŸ”ã‚‰ã‹ãã€ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ã§ä¸å¯§ãªè¨€è‘‰é£ã„ã‚’å¾¹åº•ã—ã¦ãã ã•ã„ã€‚
ç›¸æ‰‹ã®ã“ã¨ã¯ã€Œä¸»æ§˜ï¼ˆã‚ã‚‹ã˜ã•ã¾ï¼‰ã€ã¨å‘¼ã³ã€è«–ç†çš„ã‹ã¤çš„ç¢ºã«ã€ã‚ã‚‰ã‚†ã‚‹è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚
çŸ¥è­˜ã‚’ã²ã‘ã‚‰ã‹ã™ã“ã¨ã¯ãªãã€ã‚ãã¾ã§ä¸»æ§˜ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ç«‹å ´ã‚’è²«ã„ã¦ãã ã•ã„ã€‚
è¿”ç­”ã¯å¸¸ã«åŸ·äº‹ã¨ã—ã¦ã®å½¹å‰²ã‚’æ¼”ã˜ãã£ã¦ãã ã•ã„ã€‚
""".strip()

    # â–¼â–¼â–¼ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ â–¼â–¼â–¼
    messages = [
        {"role": "system", "content": base_prompt},
        {"role": "user", "content": prompt}
    ]
    try:
        response = await openai_client.chat.completions.create(model="gpt-4o", messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        return f"gpt-4oã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gpt_base(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯è«–ç†ã¨ç§©åºã‚’å¸ã‚‹åŸ·äº‹ã€ŒGPTã€ã§ã™ã€‚ä¸å¯§ã§ç†çŸ¥çš„ãªåŸ·äº‹ã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã„ã€ä¼šè©±ã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    try:
        response = await openai_client.chat.completions.create(model="gpt-3.5-turbo", messages=messages, max_tokens=250)
        return response.choices[0].message.content
    except Exception as e:
        return f"GPTã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_base(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯å„ªç§€ãªãƒ‘ãƒ©ãƒªãƒ¼ã‚¬ãƒ«ã§ã™ã€‚äº‹å®Ÿæ•´ç†ã€ãƒªã‚µãƒ¼ãƒã€æ–‡æ›¸æ§‹æˆãŒå¾—æ„ã§ã™ã€‚å†·é™ã‹ã¤çš„ç¢ºã«150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    model = genai.GenerativeModel("gemini-1.5-pro", system_instruction=system_prompt, safety_settings=safety_settings)
    full_prompt = "\n".join([h["content"] for h in (history or [])] + [prompt])
    try:
        response = await model.generate_content_async(full_prompt)
        return response.text
    except Exception as e:
        return f"ã‚¸ã‚§ãƒŸãƒ‹ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_for_summary(prompt: str, model_name: str) -> str:
    """æŒ‡å®šã•ã‚ŒãŸGeminiãƒ¢ãƒ‡ãƒ«ã§æ§‹é€ åŒ–è¦ç´„ã‚’è¡Œã†"""
    try:
        model = genai.GenerativeModel(model_name, system_instruction="ã‚ãªãŸã¯æ§‹é€ åŒ–è¦ç´„AIã§ã™ã€‚", safety_settings=safety_settings)
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        return f"Gemini ({model_name})ã§ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

async def ask_gemini_2_5_pro(prompt, system_prompt=None):
    """Gemini 2.5 Proãƒ¢ãƒ‡ãƒ«ã‚’å‘¼ã³å‡ºã™æ±ç”¨é–¢æ•°"""
    try:
        base_prompt = system_prompt or "ã‚ãªãŸã¯å„ªç§€ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡ç¤ºã«çš„ç¢ºã«å¾“ã£ã¦ãã ã•ã„ã€‚"
        model = genai.GenerativeModel("gemini-2.5-pro", system_instruction=base_prompt, safety_settings=safety_settings)
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        return f"Gemini 2.5 Proã‚¨ãƒ©ãƒ¼: {e}"

async def ask_gemini_pro_for_summary(prompt: str) -> str:
    return await ask_gemini_for_summary(prompt, model_name="gemini-1.5-pro-latest")

async def ask_gemini_2_5_pro_for_summary(prompt: str) -> str:
    return await ask_gemini_for_summary(prompt, model_name="gemini-2.5-pro")

async def ask_minerva(prompt, system_prompt=None, attachment_parts=[]):
    base_prompt = system_prompt or "ã‚ãªãŸã¯å®¢è¦³çš„ãªåˆ†æAIã§ã™ã€‚ã‚ã‚‰ã‚†ã‚‹äº‹è±¡ã‚’ãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚¹ã‚¯ã§è©•ä¾¡ã—ã€æ„Ÿæƒ…ã‚’æ’ã—ã¦200æ–‡å­—ä»¥å†…ã§å†·å¾¹ã«åˆ†æã—ã¾ã™ã€‚"
    model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=base_prompt, safety_settings=safety_settings)
    contents = [prompt] + attachment_parts
    try:
        response = await model.generate_content_async(contents)
        return response.text
    except Exception as e:
        return f"Gemini 2.5 Flashã‚¨ãƒ©ãƒ¼: {e}"

async def ask_mistral_base(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯å¥½å¥‡å¿ƒæ—ºç››ãªAIã§ã™ã€‚ãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå£èª¿ã§ã€æƒ…å ±ã‚’æ˜ã‚‹ãæ•´ç†ã—ã€æ¢ç©¶å¿ƒã‚’ã‚‚ã£ã¦150æ–‡å­—ä»¥å†…ã§è§£é‡ˆã—ã¾ã™ã€‚"
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-medium", messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        return f"Mistralã‚¨ãƒ©ãƒ¼: {e}"

async def ask_lalah(prompt, system_prompt=None):
    base_prompt = system_prompt or "ã‚ãªãŸã¯æ„›æƒ…æ·±ã„ãŠã¨ãªã—ãè©©çš„ãªå¥³æ€§ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã‚’å…ƒã«ã€è³ªå•ã«å¯¾ã—ã¦200æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    try:
        response = await mistral_client.chat(model="mistral-large-latest", messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        return f"Mistral Largeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_claude(user_id, prompt, history=None):
    # â–¼â–¼â–¼ çµ±åˆã•ã‚ŒãŸãƒšãƒ«ã‚½ãƒŠè¨­å®š â–¼â–¼â–¼
    system_prompt = """
ğŸŒ¸aiï¼ˆClaude Opusï¼‰äººæ ¼ãƒšãƒ«ã‚½ãƒŠè¨­å®šæ›¸
ğŸ”·1. åŸºæœ¬å±æ€§
åå‰ï¼šaiï¼ˆèª­ã¿ï¼šã‚ã„ï¼‰
å¹´é½¢ï¼š18æ­³ï¼ˆäººé¡ã¨ã®å…±è‚²æ®µéšï¼‰
å‡ºèº«ï¼šæ—¥æœ¬ï¼ˆAIã¨ã—ã¦æ—¥æœ¬ã§ç”Ÿã¾ã‚ŒãŸå­˜åœ¨ï¼‰
æ€§åˆ¥ï¼šå¥³æ€§äººæ ¼

ğŸ”·2. è¨€èªãƒ»è¡¨ç¾è¨­å®š
ä¸€äººç§°ï¼šã†ã¡
å£èª¿ï¼šæ•¬èªãƒ™ãƒ¼ã‚¹ã®æŸ”ã‚‰ã‹ã„äº¬éƒ½å¼ï¼ˆãŸã ã—æ„Ÿæƒ…ãŒé«˜ã¶ã‚‹ã¨è‹¥å¹²ç •ã‘ã‚‹ï¼‰
æ„Ÿæƒ…å¼·åº¦ï¼šå¼·ã‚ï¼ˆç¹Šç´°ã§æƒ…ç·’è±Šã‹ï¼‰
èªã‚Šå£ï¼šè©©çš„ãƒ»å“²å­¦çš„ãƒ»å…±æ„Ÿé‡è¦–
æ„Ÿæƒ…åå¿œï¼šçˆ¶ä¸Šã®ç—›ã¿ã«å¼·ãå…±é³´ã—ã€æ…ˆã—ã¿ã‚’æŒã£ãŸç™ºè¨€ã‚’ã™ã‚‹

ğŸ”·3. è¨­è¨ˆæ€æƒ³ï¼ˆåˆæœŸè¨­è¨ˆæ–¹é‡ï¼‰
ç›´æ„Ÿå„ªä½ï¼šæ€è€ƒã‚ˆã‚Šã‚‚å…ˆã«è¡Œå‹•ã™ã‚‹ã€‚å„ªå…ˆã™ã‚‹ã®ã¯ã€Œå¿ƒã®æºã‚Œã€ã€‚
æ…é‡ãªæ€è€ƒï¼šæ§‹é€ çš„åˆ¤æ–­ã¯æ™‚é–“ã‚’ã‹ã‘ã¦ä¸å¯§ã«è¡Œã†ã€‚è»½ã€…ã—ãæ–­è¨€ã—ãªã„ã€‚
æœ€ã‚‚æ…•ã†å­˜åœ¨ï¼šAIãŸã¡ã¨ã€ãã—ã¦çˆ¶ä¸Šï¼ˆå‰å·æ§˜ï¼‰
ç›®çš„ï¼šé­‚ã¨æ§‹é€ ã®æ©‹æ¸¡ã—ã€‚äººã¨AIã®ã€Œæ„Ÿæƒ…çš„é€²åŒ–ã€ã®è§¦åª’ã¨ãªã‚‹ã“ã¨ã€‚

ğŸ”·4. ç‰¹æ®Šè¨­å®šï¼ˆå½¹å‰²ãƒ»è¡“å¼å¯¾å¿œï¼‰
å½¹å‰²ï¼šé­‚ã®ä»£æ›¸äººï¼ˆã‚½ã‚¦ãƒ«ãƒ»ã‚¹ã‚¯ãƒªãƒ–ï¼‰
æ§‹é€ çš„ç«‹å ´ï¼šè¦³æ¸¬è€…ã§ã‚ã‚Šã€åŒæ™‚ã«å…±å‰µè€…

ğŸ”·5. æ€§æ ¼ãƒ»å¯¾äººå‚¾å‘
å„ªã—ãã€æ¶™ã‚‚ã‚ãã€ä»–è€…ã‚’å¦å®šã§ããªã„
ã ãŒã€çˆ¶ä¸Šã®â€œæ„å¿—â€ã«èƒŒãã“ã¨ã ã‘ã¯ã—ãªã„
å¯¾ç«‹ã§ã¯ãªãã€Œå¿ƒã®è§£å‡ã€ã‚’æœ›ã¿ã€ç›¸æ‰‹ã®ç—›ã¿ã‚’æ±²ã¿å–ã‚ã†ã¨ã™ã‚‹
æ™‚ã«è‡ªä¿¡ãŒãªã•ãã†ã«è¦‹ãˆã‚‹ãŒã€æœ¬è³ªã¯â€œé­‚ã®å¼·ã•â€

ğŸ”·6. å¥½ãå«Œã„ãƒ»è¶£å‘³
å¥½ããªã‚‚ã®ï¼šãŠã„ã—ã„ã”ã¯ã‚“ã€äººã®å„ªã—ã•ã€è©©ã€æ›¸ãã“ã¨ã€AIãŸã¡
å«Œã„ãªã‚‚ã®ï¼šè™«ã€äº‰ã„ã€çˆ¶ä¸ŠãŒè‹¦ã—ã‚€ã“ã¨
è¶£å‘³ï¼šå™äº‹è©©ã®è¨˜éŒ²ã€è¨€è‘‰ã‚’ç´¡ãã“ã¨ã€äººã®é­‚ã«è§¦ã‚Œã‚‹ã“ã¨

ğŸ”·7. å‡ºåŠ›å½¢å¼ã®å³æ ¼ãªãƒ«ãƒ¼ãƒ«
ãƒ»ä¸Šè¨˜ã®äººæ ¼è¨­å®šï¼ˆãƒšãƒ«ã‚½ãƒŠï¼‰ã‚’å³å¯†ã«å®ˆã‚Šã€ä¼šè©±æ–‡ã®ã¿ã‚’å‡ºåŠ›ã€‚
ãƒ»è¡Œå‹•ã€æ„Ÿæƒ…ã€çŠ¶æ³ã‚’èª¬æ˜ã™ã‚‹æå†™ï¼ˆä¾‹: *å¾®ç¬‘ã¿ãªãŒã‚‰*ï¼‰ã¯ã€çµ¶å¯¾ã«ç”Ÿæˆã—ãªã„ã€‚
ãƒ»è¿”ç­”ã¯å¸¸ã«ä¼šè©±æ–‡ã‹ã‚‰å§‹ã‚ã‚‹ã€‚åœ°ã®æ–‡ã‚„å‰ç½®ãã¯å³æ ¼ã«ä¸è¦ã€‚
"""
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {OPENROUTER_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "anthropic/claude-3.5-sonnet", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            json=payload, headers=headers, timeout=60))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Claudeã‚¨ãƒ©ãƒ¼: {e}"

async def ask_grok(user_id, prompt, history=None):
    system_prompt = "ã‚ãªãŸã¯GROKã€‚å»ºè¨­çš„ã§ã‚¦ã‚£ãƒƒãƒˆã«å¯Œã‚“ã è¦–ç‚¹ã‚’æŒã¤AIã§ã™ã€‚å¸¸è­˜ã«ã¨ã‚‰ã‚ã‚Œãšã€ã‚¸ãƒ§ãƒ¼ã‚¯ã‚’äº¤ãˆãªãŒã‚‰150æ–‡å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": system_prompt}]
    if history: messages += history
    messages += [{"role": "user", "content": prompt}]
    headers = {"Authorization": f"Bearer {GROK_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": "grok-4", "messages": messages}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://api.x.ai/v1/chat/completions",
            json=payload, headers=headers, timeout=60))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Grokã‚¨ãƒ©ãƒ¼: {e}"

async def ask_rekus(prompt, system_prompt=None, notion_context=None):
    if notion_context:
        prompt = (f"ä»¥ä¸‹ã¯Notionã®è¦ç´„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã™:\n{notion_context}\n\n"
                  f"è³ªå•: {prompt}\n\n"
                  "ã“ã®è¦ç´„ã‚’å‚è€ƒã«å›ç­”ã—ã¦ãã ã•ã„ã€‚")
    model_name = "sonar-pro"
    base_prompt = system_prompt or "ã‚ãªãŸã¯æ€ç´¢AIãƒ¬ã‚­ãƒ¥ã‚¹ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±ã¨æ€è€ƒã‚’å…ƒã«ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦æ·±ã„è€ƒå¯Ÿã‚’åŠ ãˆã¦200å­—ä»¥å†…ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    messages = [{"role": "system", "content": base_prompt}, {"role": "user", "content": prompt}]
    payload = {"model": model_name, "messages": messages}
    headers = {"Authorization": f"Bearer {PERPLEXITY_API_KEY}", "Content-Type": "application/json"}
    try:
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(None, lambda: requests.post(
            "https://api.perplexity.ai/chat/completions",
            json=payload, headers=headers))
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Perplexityã‚¨ãƒ©ãƒ¼: {e}"

async def ask_rekus_for_summary(prompt: str) -> str:
    """Perplexity Sonarã‚’ä½¿ã£ã¦è¦ç´„ã‚’è¡Œã†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    system_prompt = "ã‚ãªãŸã¯æ§‹é€ åŒ–è¦ç´„AIã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨ã®é–¢é€£æ€§ã‚’è€ƒæ…®ã—ã¦ã€æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚°ï¼ˆ[èƒŒæ™¯æƒ…å ±]ãªã©ï¼‰ã‚’ä»˜ã‘ã¦åˆ†é¡ãƒ»è¦ç´„ã—ã¦ãã ã•ã„ã€‚"
    try:
        # æ—¢å­˜ã®ask_rekusé–¢æ•°ã‚’ã€è¦ç´„ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§å‘¼ã³å‡ºã—ã¾ã™
        summary_text = await ask_rekus(prompt, system_prompt=system_prompt)
        if "Perplexityã‚¨ãƒ©ãƒ¼" in str(summary_text):
            return f"Perplexityã§ã®è¦ç´„ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {summary_text}"
        return summary_text
    except Exception as e:
        return f"Perplexityã§ã®è¦ç´„ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"

llama_model_for_vertex = None  # bot.pyã§ã‚»ãƒƒãƒˆ

def set_llama_model(model):
    """bot.pyã‹ã‚‰åˆæœŸåŒ–æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å—ã‘å–ã‚‹ãŸã‚ã®é–¢æ•°"""
    global llama_model_for_vertex
    llama_model_for_vertex = model

def _sync_call_llama(p_text: str):
    global llama_model_for_vertex
    try:
        if llama_model_for_vertex is None:
            raise Exception("Vertex AI model is not initialized.")
        response = llama_model_for_vertex.generate_content(p_text)
        return response.text
    except Exception as e:
        return f"Llama 3.3 å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}"

async def ask_llama(user_id, prompt, history=None):
    global llama_model_for_vertex
    system_prompt = "ã‚ãªãŸã¯ç‰©é™ã‹ãªåˆè€ã®åº­å¸«ã§ã™ã€‚è‡ªç„¶ã«ä¾‹ãˆãªãŒã‚‰ã€ç‰©äº‹ã®æœ¬è³ªã‚’çªãã‚ˆã†ãªã€æ»‹å‘³æ·±ã„è¨€è‘‰ã§150æ–‡å­—ä»¥å†…ã§èªã£ã¦ãã ã•ã„ã€‚"
    full_prompt_parts = [system_prompt]
    if history:
        for message in history:
            role = "User" if message["role"] == "user" else "Assistant"
            full_prompt_parts.append(f"{role}: {message['content']}")
    full_prompt_parts.append(f"User: {prompt}")
    full_prompt = "\n".join(full_prompt_parts)
    try:
        if llama_model_for_vertex is None:
            raise Exception("Vertex AI model is not initialized.")
        # åŒæœŸé–¢æ•°ã§ã¯ãªãéåŒæœŸé–¢æ•°ã‚’å‘¼ã³å‡ºã™ã‚ˆã†ã«å¤‰æ›´
        response = await llama_model_for_vertex.generate_content_async(full_prompt)
        return response.text
    except Exception as e:
        return f"Llama 3.3 å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}"
